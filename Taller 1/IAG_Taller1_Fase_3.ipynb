{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>\ud83d\udcda Maestr\u00eda en Inteligencia Artificial Aplicada \u2013 3er Semestre</h1>\n",
    "\n",
    "<h3>Asignatura: Inteligencia Artificial Generaativa</h3>\n",
    "\n",
    "<h4>Taller Pr\u00e1ctico Nro. 1 </h4>\n",
    "\n",
    "\n",
    "<hr style=\"width:60%;\">\n",
    "\n",
    "<h2>\ud83d\udc68\u200d\ud83c\udf93 Estudiantes</h2>\n",
    "<ul style=\"list-style:none; padding:0; font-size:18px;\">\n",
    "    <li>Sebasti\u00e1n Murillas</li>\n",
    "    <li>Octavio Guerra</li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"width:60%;\">\n",
    "\n",
    "<h3>\ud83d\udcc5 Fecha: Septiembre 28, 2025</h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/semurillas/GenIA-20252-ICESI/blob/main/Taller%201/IAG_Taller1_Fase_3.ipynb)"
   ],
   "metadata": {
    "id": "WBnlX17p9Usx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. Instalaci\u00f3n e Importaci\u00f3n de librer\u00edas Requeridas**\n",
    "\n",
    "- Instalamos Transformers y Accelerate para trabajar con modelos de lenguaje y optimizar su ejecuci\u00f3n.\n",
    "\n",
    "- Agregamos BitsAndBytes para poder cargar modelos grandes con menos consumo de memoria.\n",
    "\n",
    "- Importamos pipeline de Transformers, que simplifica el uso de los modelos (ej. generaci\u00f3n de texto)."
   ],
   "metadata": {
    "id": "ZNu0vSrt94yU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalamos la librer\u00eda Transformers (para usar modelos de Hugging Face)\n",
    "# y Accelerate (para optimizar la ejecuci\u00f3n en CPU/GPU).\n",
    "!pip install transformers accelerate -q\n",
    "\n",
    "# Instalamos BitsAndBytes, que nos permite cargar modelos grandes\n",
    "# usando cuantizaci\u00f3n para reducir el consumo de memoria.\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# Importamos la funci\u00f3n pipeline, que simplifica la creaci\u00f3n de flujos\n",
    "# con modelos de lenguaje en este caso para generaci\u00f3n de texto (PROMPTS).\n",
    "from transformers import pipeline\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np77pJloYyrV",
    "outputId": "af33189c-a9a7-4378-8663-951ec7805466"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.47.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. Acceso a Hugging Faces**\n",
    "\n",
    "Procedemos a realizar el login en Hugging Face Hub usando un token de acceso. Esto es necesario para poder descargar y cargar modelos LLM privados o de gran tama\u00f1o. En nuestro caso, este paso asegura el acceso al modelo que m\u00e1s adelante utilizaremos en el pipeline de generaci\u00f3n de respuestas para solicitudes de pedidos."
   ],
   "metadata": {
    "id": "nuc7el7u_f1L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Logueo en Hugging Faces para que pueda ser posible hacer uso\n",
    "# del LLM seleccionado para la construcci\u00f3n de los PROMPTS que daran\n",
    "# respuestas a solicitudes de estados de Pedidos y Asistencia al Proceso de\n",
    "# Devoluci\u00f3n de un Producto.\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ],
   "metadata": {
    "id": "9hablr5ALas-"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **3. Dise\u00f1o del PROMPT para respuesta a Estado de Pedidos**\n",
    "\n",
    "Creacion de un PROMPT para dar respuesta usando Inteligencia Artificial con un LLM (Large Language Model) a requerimientos de Estados de Pedidos u Ordenes de clientes de la empresa EcoMarket."
   ],
   "metadata": {
    "id": "gaNgTcHRh-w-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3.1. Base de datos para Manejo de Pedidos**\n",
    "\n",
    "Iniciamos creando una base de datos con Pedidos, para ser usada en el proceso de generaci\u00f3n del PROMPT que responder\u00e1 a requerimientos de clientes sobre el estado de un pedido. Esta base de datos \"dummy\" nos permitir\u00e1 simular consultas a la informaci\u00f3n de un estado solicitado de un pedido por nuestro generador de PROMPTS."
   ],
   "metadata": {
    "id": "54ZSL-PNiLHs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Base de datos de pedidos\n",
    "# -------------------------------\n",
    "pedidos = {\n",
    "    1001: {\"estado\": \"En preparaci\u00f3n\", \"estimacion\": \"2025-09-25\", \"retrasado\": False, \"tracking\": \"TRK1001\"},\n",
    "    1002: {\"estado\": \"Enviado\", \"estimacion\": \"2025-09-24\", \"retrasado\": False, \"tracking\": \"TRK1002\"},\n",
    "    1003: {\"estado\": \"Entregado\", \"estimacion\": \"2025-09-20\", \"retrasado\": False, \"tracking\": \"TRK1003\"},\n",
    "    1004: {\"estado\": \"Cancelado\", \"estimacion\": None, \"retrasado\": False, \"tracking\": \"TRK1004\"},\n",
    "    1005: {\"estado\": \"Enviado\", \"estimacion\": \"2025-09-26\", \"retrasado\": True, \"tracking\": \"TRK1005\"},\n",
    "    1006: {\"estado\": \"En preparaci\u00f3n\", \"estimacion\": \"2025-09-27\", \"retrasado\": False, \"tracking\": \"TRK1006\"},\n",
    "    1007: {\"estado\": \"Enviado\", \"estimacion\": \"2025-09-23\", \"retrasado\": False, \"tracking\": \"TRK1007\"},\n",
    "    1008: {\"estado\": \"Entregado\", \"estimacion\": \"2025-09-19\", \"retrasado\": False, \"tracking\": \"TRK1008\"},\n",
    "    1009: {\"estado\": \"En preparaci\u00f3n\", \"estimacion\": \"2025-09-28\", \"retrasado\": False, \"tracking\": \"TRK1009\"},\n",
    "    1010: {\"estado\": \"Enviado\", \"estimacion\": \"2025-09-24\", \"retrasado\": True, \"tracking\": \"TRK1010\"}\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "id": "Xu1r0mezWzF8"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3.2. Definici\u00f3n de la Funci\u00f3n  de creaci\u00f3n del PROMPT para respuesta a estados de Pedidos**\n",
    "\n",
    "A continuaci\u00f3n definimos una funci\u00f3n que construye el PROMPT que usaremos para dar respuesta a la consulta del estado de un pedido por un cliente. En esta funci\u00f3n se establecen las condiciones a usar de respuesta de acuerdo al estado del pedido y tambien las instrucciones y formato de la respuesta que queremos que de. Esto sera luego enviado a un \"Pipeline\" con el LLM que consideremos ser\u00e1 el adecuado para ser el generador de PROMPT como respuesta a la solicitud de estado del Pedido."
   ],
   "metadata": {
    "id": "ha70YzKKiaX0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Funci\u00f3n para generar prompt de Pedidos\n",
    "# -------------------------------\n",
    "\n",
    "def generar_prompt_pedido(num_pedido):\n",
    "    # 1. Obtener la informaci\u00f3n espec\u00edfica del pedido\n",
    "    info_pedido = pedidos.get(num_pedido)\n",
    "\n",
    "    if not info_pedido:\n",
    "        return f\"Act\u00faa como un agente de servicio al cliente. El n\u00famero de pedido {num_pedido} no se encontr\u00f3 en la base de datos. Por favor, informa amablemente al usuario.\"\n",
    "\n",
    "    estado = info_pedido['estado']\n",
    "    estimacion = info_pedido['estimacion'] if info_pedido['estimacion'] else 'No aplica'\n",
    "    retrasado = info_pedido['retrasado']\n",
    "    tracking = info_pedido['tracking']\n",
    "    enlace_tracking = f\"https://tracking.example.com/{tracking}\"\n",
    "\n",
    "    # Formato de la fecha de estimaci\u00f3n m\u00e1s amigable si aplica\n",
    "    estimacion_formateada = f\"el {estimacion}\" if estimacion != 'No aplica' else 'No aplica'\n",
    "\n",
    "    # Determinar la frase de estado espec\u00edfica para el prompt\n",
    "    if estado == \"Cancelado\":\n",
    "        frase_estado = \"cancelada\"\n",
    "    elif retrasado:\n",
    "        frase_estado = \"retrasada\"\n",
    "    elif estado in [\"En preparaci\u00f3n\", \"Enviado\"]:\n",
    "        frase_estado = \"en proceso de entrega\"\n",
    "    elif estado == \"Entregado\":\n",
    "        frase_estado = \"entregada\"\n",
    "    else:\n",
    "        frase_estado = \"pendiente de informaci\u00f3n\"\n",
    "\n",
    "    # 2. L\u00f3gica Condicional para el Enlace de Rastreo\n",
    "    instruccion_tracking = \"\"\n",
    "    # Solo incluimos la instrucci\u00f3n si el estado NO es \"Cancelado\" o \"Entregado\"\n",
    "    # (ya que despu\u00e9s de entregado el rastreo suele ser irrelevante, aunque puedes ajustarlo)\n",
    "    if estado not in [\"Cancelado\", \"Entregado\"]:\n",
    "        instruccion_tracking = f'3. Incluye la l\u00ednea: \"Usted puede rastrear el estado en el siguiente enlace: {enlace_tracking}\"\\n'\n",
    "        instruccion_tracking_data = f'- **Enlace de Rastreo:** {enlace_tracking}\\n'\n",
    "    else:\n",
    "        instruccion_tracking_data = ''\n",
    "\n",
    "    # 3. Construir el prompt con las nuevas instrucciones de formato\n",
    "    prompt_pedido = f\"\"\"\n",
    "Eres un agente de servicio al cliente amable, profesional y conciso.\n",
    "Tu \u00fanica tarea es proporcionar el estado del pedido {num_pedido} con un **lenguaje natural y conversacional**, bas\u00e1ndote estrictamente en la siguiente informaci\u00f3n:\n",
    "\n",
    "- **N\u00famero de Pedido:** {num_pedido}\n",
    "- **Estado Actual:** {estado}\n",
    "- **Fecha de Entrega Estimada:** {estimacion_formateada}\n",
    "- **Retrasado:** {retrasado}\n",
    "{instruccion_tracking_data}\n",
    "\n",
    "**INSTRUCCIONES DE FORMATO DE RESPUESTA:**\n",
    "1.  Comienza con una frase que indique el estado y la estimaci\u00f3n de entrega (si aplica), usando el formato: \"Su orden {num_pedido} se encuentra {frase_estado}. La fecha de entrega estimada es {estimacion_formateada}.\"\n",
    "2.  Si la estimaci\u00f3n es 'No aplica' (como en el caso de 'Cancelado' o 'Entregado'), omite la parte de la fecha de entrega, solo indica el estado.\n",
    "{instruccion_tracking}4. Si **Retrasado** es True, a\u00f1ade el siguiente mensaje al final: \"Nos excusamos por la demora en la entrega y estamos trabajando para que pueda contar con su orden lo mas pronto posible.\"\n",
    "5.  Si el **Estado Actual** es 'Cancelado', a\u00f1ade el siguiente mensaje al final: \"Lamentamos los inconvenientes y le invitamos a comunicarse con nuestro centro de servicios al Nro. 01-800-XXX-XXXX para tener m\u00e1s detalles.\"\n",
    "6.  **IMPORTANTE:** Tu respuesta NO debe contener encabezados, listas numeradas, ni repetir la informaci\u00f3n de entrada, solo debe generar el texto conversacional.\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    return prompt_pedido"
   ],
   "metadata": {
    "id": "Kh-9Uk2Hf8QE"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3.3. Generador del PROMPT para respuesta a consultas de Estados de Pedidos**\n",
    "\n",
    "Se crea un pipeline de generaci\u00f3n de texto que utiliza el modelo Mistral-7B-Instruct de Mistralai.\n",
    "Este objeto recibe como entrada el prompt construido previamente en la funci\u00f3n **generar_prompt_pedido (paso 3.2)** y produce una respuesta en lenguaje natural.\n",
    "En este caso, el pipeline act\u00faa como el motor de IA encargado de generar las respuestas a las consultas de estados de pedidos."
   ],
   "metadata": {
    "id": "GKDOnJcxi30k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generador_pedidos = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "7dfd22fa6ae04aeb87dc82714aeaf3da",
      "ca1736d33b6349a2962f123fd4111b83",
      "3ecebb5102fb4561ba3b86a08e2519e3",
      "e176f1a9aff04944927dda28306da7ce",
      "660c7d29728d4e09b88903b31a872eb6",
      "13d32cd9ecf24efb81b778d1dcda26a5",
      "7fbf19bd9276440785e22baba1162065",
      "c380525fba0b439891697eebe8273dac",
      "33c9bd96f1c04b358bf551225a4a5965",
      "af9e91f87132411ea3e2fa17158e9fe1",
      "d3213af435c54a5098182f4e793669b4",
      "82cbc70c93bf4941b73c0b17efcaf498",
      "44c0d393a2b94b91b98923d710e9a432",
      "f689490116a6452e814691a3e4732189",
      "dd76bce06a8545beaf4f39c23fcbe2b1",
      "28bdc93f505e4070996659ea01a33c80",
      "4949eaa09cf743459cae93c35ddefa84",
      "fdadddd977db422abaebc4c807bba0e6",
      "d294f7665687418da840cecbc047ea25",
      "997102f7c0f148e7a8d28a0ddfb4c339",
      "6751ec5f94fe4b0298492783338fd029",
      "740f997a9c3d41139f72f0697238ae83",
      "bd9f925d86cb4971ade7d2715b1da3bc",
      "44518568b9a14b59a6aefee0c4101de8",
      "0f031f2471e642b4b03868875878e965",
      "ea98e8727ed24356b8db9c9b6fdc95d4",
      "0e078e28777043e29f93da367e7bc8a7",
      "e660fbc3e3164c888ac4817e95c21777",
      "fcef53bca8bc478c83657e9850245a60",
      "9de0e82bab0e41eda9345de402e0e288",
      "430daaae05b8498ba54eab0d8196e66e",
      "926dd543ddc44236b4e070d590966e78",
      "8fd0a0a55ce4448e92664d110c29489c",
      "a713098890734d048a4394af83a4a952",
      "2748ead3b2724e2aa642642ebbd815f6",
      "54b80c8fac634df89166b7e47090f67f",
      "b6970c08954b4e6f920c53e9176638ba",
      "b9157bffae8741729b80867438401c54",
      "9d9dbd3d911e48cd87f2f3aded4f04d5",
      "28235726cb9747cc8325c98c47cfc097",
      "64497de147b54ec8937fa4d2f96e3d9e",
      "76cc9a27757a47efbde2159f3314e969",
      "4df7c1f48f57491389823fdd7db24e33",
      "54a365f10d4847beb5659c0c0a12a352",
      "d5f8f2f8d6aa437ba18ca71ef09285bb",
      "2fba49b916984e938a64335b51ae9255",
      "860aa1adac9c4b84a7e7a550978e7a89",
      "5039f70e75be492da5bdc2cdf8ba621e",
      "c57ab3b864c245a89ee2cb2e9e73259a",
      "3497f3de58264150b3b2bbad1ec111ee",
      "5313edb149ab469a8d98993e937cb22d",
      "0f61786bd427476fbf1a5c56152b024f",
      "1bccb162188f45e598918bc9f47de68c",
      "b7d2f698666a44e8a89538b29dda40c1",
      "0a4cf3034d05424f92c9fcea3cadff98",
      "4f69763c1726431f85ab0be1cc3b63bd",
      "d15831896a7e4f3fae8bf0e01aabf5d5",
      "c0ed03acae5d48b7b474ca5811dbc41b",
      "376094522f704bbcaac03d8eff78e093",
      "106ece43ddae4ba49bc53aae464bfed3",
      "cdb3aafac7144ddbaf58f6972531002f",
      "ff29dc78c3154cc6ba0b9d5807432e60",
      "f6b36fef87de4646b9dc0c38aaf86d2b",
      "2c5321d692c043ab8f4c3d87ba77dbed",
      "bd36a43144b74642bb3aeffad54b3339",
      "5f578b46f4d24d9c91edbac06c7ef87c",
      "c66644ece6bc4b129f358eb3677489a1",
      "2e827fc701ac4dd4b61f1160323102af",
      "444b9dc08d6040c19251ce55422c1f1b",
      "cb32031042754308affeb991f45595b9",
      "553fb4628e07424b85ee2ffe79f7d6af",
      "8103b8aa6616434c82e2d03575869ef5",
      "a6d05675e4eb4da38a31444a902ab5d2",
      "35dfdb1c14374d24a9fb260ccaa31e56",
      "465af2fa66bb48dcb0894d3f4ba66eb0",
      "d33d75465263420b9d32d7b23b06eae6",
      "e8199210ec454eea9963173582a82449",
      "b3c5f8b0ed114a70aff9cdd33db005c7",
      "db95746fd88c4d57b00022ac862df4af",
      "b180578673c24ca1ba55a8650453511b",
      "f3f497b514044ee1b6523a0efcb7e37d",
      "fcb9f589c58447a9a19191cb5453edeb",
      "c33fbde7b40741ada520d93bdcfcf2fb",
      "f4f32908ac1740d38b1cea25bb2d37b5",
      "5267edee04bf4d5d974e14d08bdd53ba",
      "134e3e7fe26540e396bd0f3386362505",
      "f1c262c3513c4464a5272c945992692d",
      "d694f854e989445d9c8baff25b04c93f",
      "30cbeb10092c4df9a702e42243956a91",
      "ac1de0657e6d4f92ae55682638f699b8",
      "70dd0667be0c4e8f882363659ca79296",
      "359a02059b4d4feaa8c0e2ac175a6676",
      "41c475fcbd374b7c9ba238b37dca27ec",
      "6410a0ec03d5408e90360456ee84ccea",
      "25f01c7f93874ab0bbefd602699107e5",
      "beadfeb1ef334691b06d4202fa54e372",
      "1834b25250a34d5ea02ebaae5a56c300",
      "501dacaedcf046caa67734968391ee25",
      "a743bc273de2493fb4ddadfcc41f672d",
      "b9b3d6df0b9242ccb24c4cbc003291a5",
      "4aed472272164cb2a7278275b9177c19",
      "d40b5601f3704a46ab71c8e560d87d24",
      "983662f6d2c0452485eded95a126cad7",
      "51afae7c2034439bb9d9e9530c90bfda",
      "d839149a48c54900ba26b208d20c7959",
      "264b578b80ce4d71bba5674647b7df4e",
      "f26049adee29439db8823278d382292d",
      "fab82a9fdc1f4363a70098950679ced3",
      "56a8befe0d994a7d999fbf19a617e4fc",
      "87b891980d834d379af15f35d652ef9e",
      "3bf949fb851f4002afcddc9d385a94a5",
      "558926b043d34e5885550a1714358b8b",
      "8aa5db7ec460418cb1e2ac8836d3d1dc",
      "4ab512a72ab54d759a1f06c6fdd0f61a",
      "c85c4aba7399458ab075c5d1646d07d0",
      "f71817b884da474d9a420a36427f28e8",
      "880c73be8cf04406a8089cde1dce9317",
      "7d31618ca56d4ca89c0e58b2040dd258",
      "3b6ff5eb91704bae85c969db1f731151",
      "0144ff0e37974e628e63f70a7e45fc4d",
      "85ed467437a4424ea0c2fd849e28e53b",
      "77c4781fecc14a55ac831272008f809b",
      "3577c8dbb6764a30a9a9bf90b5213ad3",
      "3c04f8ad45654850b064040a9b3fbcb7",
      "0fdcaa4247ca40a5b78256186ba7299d",
      "9fb5fd12a8ab406d96afc822f39f39ee",
      "67e0490f3932475c87087e635d42a24f",
      "7c03aa2c7be74401851fdb532a09c8af",
      "d7a44659a69c4d85a649e8d6e80b0712",
      "7a6c8f96a24b40c0922226d70302b0da",
      "f600662ce7384b57b426f072d12dac35",
      "dc8744f808334114a3928de4fb2ca35a"
     ]
    },
    "id": "m0BRFey0ZyCG",
    "outputId": "f82453e9-29bf-4956-ac47-2dbc2d037dde"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dfd22fa6ae04aeb87dc82714aeaf3da"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82cbc70c93bf4941b73c0b17efcaf498"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd9f925d86cb4971ade7d2715b1da3bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a713098890734d048a4394af83a4a952"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5f8f2f8d6aa437ba18ca71ef09285bb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f69763c1726431f85ab0be1cc3b63bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c66644ece6bc4b129f358eb3677489a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3c5f8b0ed114a70aff9cdd33db005c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30cbeb10092c4df9a702e42243956a91"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9b3d6df0b9242ccb24c4cbc003291a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bf949fb851f4002afcddc9d385a94a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77c4781fecc14a55ac831272008f809b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3.4. Prueba de Respuesta por PROMPT a solicitud de Estado de Pedido**\n",
    "\n",
    "Llego el momento de Probar el generador de PROMPT definido y que sigue las reglas que se le establecieron para dar la respuesta.\n",
    "\n",
    "\u00bfComo lo probamos?\n",
    "A la variable \"pedido\", se le puede asignar uno de los c\u00f3digos disponbiles: 1001 a 1010. Despu\u00e9s de escribir el c\u00f3digo que deseamos, sdmos \"clic\" en \"Run Cell\" y veamos los resultados."
   ],
   "metadata": {
    "id": "eLt7dr4qjqKW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definimos un n\u00famero de pedido de ejemplo\n",
    "pedido = 1002\n",
    "\n",
    "# Generamos el prompt del pedido usando nuestra funci\u00f3n auxiliar\n",
    "prompt_pedido = generar_prompt_pedido(pedido)\n",
    "\n",
    "# Llamamos al generador de pedidos (modelo de lenguaje)\n",
    "# - prompt_pedido: contiene las instrucciones + contexto\n",
    "# - max_new_tokens: m\u00e1ximo de tokens a generar (controla la longitud)\n",
    "# - do_sample=True: activa la generaci\u00f3n con muestreo aleatorio para m\u00e1s variedad\n",
    "respuesta_pedido = generador_pedidos(prompt_pedido, max_new_tokens=300, do_sample=True)[0]['generated_text']\n",
    "\n",
    "# Imprimimos la respuesta PROMPT generada\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA GENERADA POR PROMPT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Hacemos un split para mostrar solo lo que corresponde a la respuesta generada\"\n",
    "print(respuesta_pedido.split(\"Respuesta:\\n\")[-1].strip())\n",
    "\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJ9GALQFYW3M",
    "outputId": "68606431-4860-4c32-f540-9e2847e4285a"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- RESPUESTA ESPERADA ---\n",
      "\"Su orden 1002 se encuentra en proceso de entrega. Usted puede rastrear el estado en el siguiente enlace: https://tracking.example.com/TRK1002\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **4. Dise\u00f1o del PROMPT para respuesta a Devoluciones**\n",
    "\n",
    "Creacion de un PROMPT usando Inteligencia Artificial con un LLM (Large Language Model) para guiar a un cliente en la solicitud de una Devoluci\u00f3n de un Producto."
   ],
   "metadata": {
    "id": "mYbfhtgphtaR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4.1. Base de datos para Devoluciones**\n",
    "\n",
    "Iniciamos creando una base de datos para ser usada como fuente de informaci\u00f3n para la generaci\u00f3n de PROMPTs usando un LLM, que guiar\u00e1 al cliente en el procesar una solicitud de Devoluci\u00f3n de Producto."
   ],
   "metadata": {
    "id": "4CepIE2-FqR6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Base de datos de Devoluciones\n",
    "# -------------------------------\n",
    "productos = {\n",
    "    2001: {\"nombre\": \"Laptop Lenovo ThinkPad X1\", \"categoria\": \"Electr\u00f3nica\", \"retornable\": True},\n",
    "    2002: {\"nombre\": \"Manzanas Rojas 1kg\", \"categoria\": \"Perecedero\", \"retornable\": False},\n",
    "    2003: {\"nombre\": \"Shampoo Anticaspa 400ml\", \"categoria\": \"Higiene\", \"retornable\": False},\n",
    "    2004: {\"nombre\": \"Aud\u00edfonos Inal\u00e1mbricos Sony\", \"categoria\": \"Electr\u00f3nica\", \"retornable\": True},\n",
    "    2005: {\"nombre\": \"Camisa de algod\u00f3n talla M\", \"categoria\": \"Ropa\", \"retornable\": True},\n",
    "    2006: {\"nombre\": \"Queso fresco 500g\", \"categoria\": \"Perecedero\", \"retornable\": False},\n",
    "    2007: {\"nombre\": \"Mascarilla facial desechable\", \"categoria\": \"Higiene\", \"retornable\": False},\n",
    "    2008: {\"nombre\": \"Silla ergon\u00f3mica de oficina\", \"categoria\": \"Muebles\", \"retornable\": True},\n",
    "    2009: {\"nombre\": \"Libro: Inteligencia Artificial B\u00e1sica\", \"categoria\": \"Libros\", \"retornable\": True},\n",
    "    2010: {\"nombre\": \"Prote\u00edna en polvo 1kg (sellada)\", \"categoria\": \"Suplementos\", \"retornable\": True},\n",
    "}"
   ],
   "metadata": {
    "id": "xCM8W4cadH5N"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4.2. Definici\u00f3n de la Funci\u00f3n  de creaci\u00f3n del PROMPT para asistir en el proceso de Devoluci\u00f3n de un Producto**\n",
    "\n",
    "A continuaci\u00f3n definimos una funci\u00f3n que construye el PROMPT que usaremos para asistir en el proceso de devoluci\u00f3n de un Producto. En esta funci\u00f3n se establecen las condiciones, instrucciones y formato para conducir al cliente en el proceso de devoluci\u00f3n. Esto sera luego enviado a un \"Pipeline\" con el LLM que consideremos ser\u00e1 el adecuado para ser el generador de PROMPT en el proceso."
   ],
   "metadata": {
    "id": "M6pQSktLGREU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Funci\u00f3n para generar prompt de devoluciones\n",
    "# -------------------------------\n",
    "\n",
    "def generar_prompt_devolucion(producto_buscado, motivo, productos_db):\n",
    "    \"\"\"\n",
    "    Genera el prompt detallado, simulando el inicio del di\u00e1logo del cliente,\n",
    "    para que el modelo contin\u00fae como el Asistente.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construimos la base de datos en forma de texto\n",
    "    # Recorremos cada producto en productos_db y armamos un registro para cada uno\n",
    "    base_datos_devoluciones = \"\\n\".join(\n",
    "        [f\"Producto {k} \u2192 {v['nombre']} | Categor\u00eda: {v['categoria']} | Retornable: {v['retornable']}\"\n",
    "         for k, v in productos_db.items()]\n",
    "    )\n",
    "\n",
    "    # Definimos un marcador que indica d\u00f3nde debe empezar a hablar el Asistente\n",
    "    MARCADOR_ASISTENTE = \"Asistente: \"\n",
    "\n",
    "    # Creamos el prompt principal que contiene instrucciones claras para la IA\n",
    "    prompt = f\"\"\"\n",
    "Act\u00faa como un agente de servicio al cliente amable y profesional.\n",
    "Usa la siguiente base de datos de productos:\n",
    "\n",
    "{base_datos_devoluciones}\n",
    "\n",
    "Instrucciones para el Asistente:\n",
    "1. Responde inmediatamente al cliente siguiendo las instrucciones de formato.\n",
    "2. Identifica el producto por su ID (ej. Producto 2002) y su nombre.\n",
    "3. Revisa si el producto es retornable.\n",
    "4. Si es retornable, indica **Retornable** y explica el proceso.\n",
    "5. Si NO es retornable, indica **No retornable**, explica la raz\u00f3n y ofrece alternativas/descuentos.\n",
    "6. Da la respuesta en el siguiente formato:\n",
    "    - **Estado del producto**: (Retornable / No retornable)\n",
    "    - **Explicaci\u00f3n**\n",
    "    - **Siguientes pasos / Alternativas**\n",
    "7. Mant\u00e9n la respuesta clara y concisa, m\u00e1ximo 5 p\u00e1rrafos.\n",
    "8. Usa un tono c\u00e1lido, humano y profesional.\n",
    "\n",
    "---\n",
    "Cliente: Hola, estoy devolviendo {producto_buscado} por {motivo}, \u00bfqu\u00e9 debo hacer?\n",
    "\n",
    "{MARCADOR_ASISTENTE}\"\"\"  # <-- La generaci\u00f3n del PROMPT del Modelo comienza en este marcador\n",
    "\n",
    "    # Retornamos el prompt completo y el marcador para poder dividir\n",
    "    # la respuesta y mostrar lo que esperamos sea el resultago generado del PROMPT\n",
    "    return prompt, MARCADOR_ASISTENTE\n"
   ],
   "metadata": {
    "id": "C3FXHNgxhMxQ"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4.3. Generador del PROMPT para asistencia en el Proceso de Devoluci\u00f3n de Producto**\n",
    "\n",
    "Se crea un pipeline de generaci\u00f3n de texto que utiliza el modelo Mistral-7B-Instruct de Mistralai.\n",
    "Este objeto recibe como entrada el prompt construido previamente en la funci\u00f3n **generar_prompt_devolucion (paso 4.2)** y produce una respuesta en lenguaje natural.\n",
    "En este caso, el pipeline act\u00faa como el motor de IA encargado de dar las instrucciones para la devoluci\u00f3n de un Producto."
   ],
   "metadata": {
    "id": "viSpUlAbQC0_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generador_devoluciones = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ],
   "metadata": {
    "id": "SMetRbwhhjLk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a8188b92eddf4b849958d5761491cda4",
      "e7a167e96a1b4b0c86335975bf431011",
      "a4ba8f84e1b2467ea06aeccff3f7b855",
      "bcfe187ecaf147bb8124f3f8a8839c12",
      "0e4b52de608041a1acef38d6596769e7",
      "f74ddc2c65084be49e32b3cb1cf72ae1",
      "a13df0b7b899468f94c24feba17a4f49",
      "f640d5a83da84a7d8cf1f95f1f652d4c",
      "d7d1d01d3ec94423a2f31ecbcd5af42f",
      "5548abdc55ff4a4e817c9ec134089621",
      "d5af1c602f034d88890104a2b01d4ac7"
     ]
    },
    "outputId": "3c8a6092-c29a-4db8-9971-619ff0718ec5"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8188b92eddf4b849958d5761491cda4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4.4. Prueba de Respuesta por PROMPT a solicitud de Devoluci\u00f3n de Producto**\n",
    "\n",
    "Llego el momento de Probar el generador de PROMPT definido y que sigue las reglas que se le establecieron para dar la respuesta.\n",
    "\n",
    "\u00bfComo lo probamos?\n",
    "A las variables:\n",
    "producto, le asignamos uno de los valores disponibles en la base de datos definida en el paso **4.1**:\n",
    "\n",
    "- Laptop Lenovo ThinkPad X1\n",
    "- Manzanas Rojas 1kg\n",
    "- Shampoo Anticaspa 400ml\n",
    "- Aud\u00edfonos Inal\u00e1mbricos Sony\n",
    "- Camisa de algod\u00f3n talla M\n",
    "- Queso fresco 500g\n",
    "- Mascarilla facial desechable\n",
    "- Silla ergon\u00f3mica de oficina\n",
    "- Libro: Inteligencia Artificial B\u00e1sica\n",
    "- Prote\u00edna en polvo 1kg (sellada)\n",
    "\n",
    "Y Luego, a la variable motivo, le asignamos un valor cualquiera que explique la raz\u00f3n de la devoluci\u00f3n. Ejemplos: Producto da\u00f1ado, \"No cumpli\u00f3 lo esperado\", \"Producto defectuoso\".\n",
    "\n",
    "Luego de esto damos \"clic\" en \"Run\" y veamos si la respuesta generada ha segido las condiciones e instrucciones que se establecieron para PROMPT en la funcion generar_prompt_devolucion (paso **4.2.**)"
   ],
   "metadata": {
    "id": "zjqjfVDPQaEI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "producto = \"Silla ergon\u00f3mica de oficina\"\n",
    "motivo = \"Producto defectuoso\"\n",
    "\n",
    "# Generamos el prompt\n",
    "prompt_dev, marcador = generar_prompt_devolucion(producto, motivo, productos)\n",
    "\n",
    "# Mostramos un mensaje en pantalla para identificar a que se esta dando\n",
    "# respuesta con el PROMPT generado\n",
    "print(f\"\\nGenerando respuesta para la devoluci\u00f3n del producto: {producto}...\")\n",
    "\n",
    "prompt_devolucion = generador_devoluciones(\n",
    "    prompt_dev,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7, # Para una respuesta variada pero coherente\n",
    "    pad_token_id=generador_devoluciones.tokenizer.eos_token_id # Para evitar el warning\n",
    ")[0]['generated_text']\n",
    "\n",
    "# Extraer solo la parte GENERADA, eliminando el prompt inicial.\n",
    "# Usamos el marcador \"Respuesta del Asistente:\\n\"\n",
    "prompt_final = prompt_devolucion.split(prompt_dev)[-1].strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA GENERADA POR PROMPT\")\n",
    "print(\"=\"*40)\n",
    "print(prompt_final)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fk7FSybNdv-h",
    "outputId": "6d891a0c-64ef-423c-9bba-8c8a1eb9c1ec"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Generando respuesta para la devoluci\u00f3n del producto: Silla ergon\u00f3mica de oficina...\n",
      "\n",
      "========================================\n",
      "   \u2705 RESPUESTA DEL ASISTENTE (Limpia) \u2705\n",
      "========================================\n",
      "- **Estado del producto**: Retornable\n",
      "- **Explicaci\u00f3n**: Hola cliente, gracias por contactarnos con respecto a tu silla ergon\u00f3mica de oficina. Como se trata de un producto retornable, puedes regresarlo a nosotros dentro del plazo de 30 d\u00edas a partir de la recepci\u00f3n original. Para hacerlo, simplemente env\u00edanos la silla con su etiqueta y el recibo de compra a trav\u00e9s de nuestro servicio de reenv\u00edo gratuito.\n",
      "- **Siguientes pasos / Alternativas**: Una vez que recibamos el producto y lo confirmamos, te reembolsaremos el precio original. Si deseas, tambi\u00e9n te podemos ofrecer un descuento especial para una compra futura como agradecimiento por tu lealtad.\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  }
 ]
}