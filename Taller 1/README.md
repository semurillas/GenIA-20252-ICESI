<h1> <img width="207" height="112" alt="image" src="https://github.com/user-attachments/assets/89fd906b-04fb-4d4f-b5e6-8375083a8a01" /></h1>
<h1>üìö Maestr√≠a en Inteligencia Artificial Aplicada ‚Äì 3er Semestre</h1>

<h3>Asignatura: Inteligencia Artificial Generativa</h3>

<h3>Taller Pr√°ctico Nro. 1 </h3>

<hr style="width:60%;">

<h3>üë®‚Äçüéì Estudiantes</h3>
<ul style="list-style:none; padding:0; font-size:18px;">
    <li>Sebasti√°n Murillas</li>
    <li>Octavio Guerra</li>
</ul>

<hr style="width:60%;">

<h3>üìÖ Fecha: Septiembre 26, 2025</h3>

---
# Fase I - Propuesta de Arquitectura IAG para Optimizaci√≥n del servicio al Cliente de la compa√±ia EcoMarket


Tras revisar la literatura y la documentaci√≥n sobre Modelos de Lenguaje Grande (LLM) y la teor√≠a de la Inteligencia Artificial Generativa, nosotros proponemos una soluci√≥n **H√≠brida**. Este enfoque busca mitigar el impacto negativo de los altos tiempos de respuesta (24 horas en promedio) en la m√©trica de **Satisfacci√≥n del Cliente** de la compa√±ia **EcoMarket**, en conjunto con un manejo costo-eficiencia, que es importante para la compa√±ia.

---

## 1. Arquitectura General

Nuestro Modelo H√≠brido est√° compuesto por:

1. **Modelo Fine Tuned LLM**, para atender el **80%** de las consultas repetitivas. Ejemplos: GPT-4omini, Gemini 1.5 Flash, Mistral 7B Instruct 
2. **Modelo LLM** de prop√≥sito general, para atender las consultas o preguntas complejas (**20%**). Ejemplos: GPT-4, Claude 3.5 Sonnet.

### ¬øPor qu√©?

Esta arquitectura permite que el modelo Fine-Tuned se enfoque en **eficiencia**, mientras que el LLM aporta **capacidad de comprensi√≥n** en consultas complejas.  
Esto resolver√° el problema m√°s importante de EcoMarket: **reducir los tiempos de respuesta que hoy son de  24 horas  a minutos** con el uso de LLM .  

En cuanto a costos, usar un esquema H√≠brido permite:  
- Operar con un **presupuesto moderado** para resolver la parte repetitiva.  
- **Estimar un costo l√≠mite m√°ximo** al usar un LLM en los casos complejos.  

La elecci√≥n del modelo se basa en un an√°lisis de **costo-beneficio** centrado en la tarea espec√≠fica que debe resolver cada componente.

---

### 1.1. Para el 80% de Consultas (Repetitivas: Pedidos, Devoluciones, Cat√°logo)

**Modelo Elegido (Ejemplo):** LLM Ligero y Open Source (e.g., *Mistral 7B*) + Afinamiento (Fine-Tuning)

| Criterio               | Justificaci√≥n |
|-------------------------|---------------|
| **Necesidad de Precisi√≥n** | Alta. Una respuesta incorrecta sobre el estado de un pedido (ej: "entregado" cuando est√° "en camino") es cr√≠tica para la satisfacci√≥n. |
| **Ventaja del Afinamiento** | Al aplicar Fine-Tuning sobre un modelo ligero base (como Mistral 7B), el modelo aprende el vocabulario, la estructura de las preguntas frecuentes y el tono de EcoMarket. Esto permite rapidez y precisi√≥n en su dominio. |
| **Costo Operacional** | Bajo. Los modelos ligeros son m√°s baratos de alojar en infraestructura propia o en la nube. Solo se activan cuando se requieren, manejando eficientemente el alto tama√±o de preguntas que recibir√°. |

---

### 1.2. Para el 20% de Consultas (Complejas: Quejas, Problemas, Sugerencias)

**Modelo Elegido (Ejemplo):** LLM de Prop√≥sito General de Alto Rendimiento (e.g., *GPT-4o, Gemini 1.5 Pro*)

| Criterio               | Justificaci√≥n |
|-------------------------|---------------|
| **Necesidad de Fluidez/Razonamiento** | Alta. Estas consultas exigen comprender el contexto emocional (empat√≠a), sintetizar informaci√≥n de m√∫ltiples fuentes y seguir instrucciones complejas. |
| **Ventaja de Rendimiento** | Modelos como GPT-4o y Gemini 1.5 Pro poseen razonamiento en cadena (*chain-of-thought*) y uso de herramientas (*Function Calling*). Esto es esencial para clasificar correctamente la queja y determinar a qu√© agente o departamento derivarla, proporcionando al humano un resumen pre-analizado. |
| **Costo Operacional** | Moderado/Alto. El costo por token es mayor, pero al representar solo el 20% del volumen de tr√°fico, la inversi√≥n se justifica porque impacta directamente en la **Satisfacci√≥n del Cliente**. |

---

## 2. Arquitectura Propuesta

Nuestra arquitectura es un **sistema de Orquestaci√≥n** con **decisi√≥n basada en el tr√°fico**, combinando modelos Fine-Tuned con LLM de Prop√≥sito General.

```mermaid
graph TD
    subgraph "Entrada de Consulta"
        A["Cliente: Chat, Email, RRSS"] --> B["Orquestador / Router"];
    end

    subgraph "Clasificaci√≥n - Router Ligero"
        B -- "80% Consultas Repetitivas" --> C["Pedidos, Devoluciones, Cat√°logo"];
        B -- "20% Consultas Complejas" --> G["Quejas, Problemas, Sugerencias"];
    end

    subgraph "Ruta 80% - Consultas Repetitivas"
        C --> D["LLM Fine-Tuned - Ej: Mistral 7B, LLaMA 2-7B"];
        D --> E["RAG conectado a BD de EcoMarket"];
        E --> F["Genera Respuesta Personalizada"];
        F --> Z["Respuesta Autom√°tica Enviada al Cliente"];
    end

    subgraph "Ruta 20% - Consultas Complejas"
        G --> H["LLM Potente - Ej: GPT-4o, Gemini 1.5 Pro, Claude 3 Opus"];
        H --> I["Function Calling / Integraci√≥n CRM & Tickets"];
        I --> J["Genera Resumen o Acci√≥n"];
        J --> K["Agente Humano Ajusta y Env√≠a"];
    end

    %% Estilos
    style A fill:#D4E6F1,stroke:#333,stroke-width:2px;
    style B fill:#AED6F1,stroke:#333,stroke-width:2px;

    style C fill:#C0F6C0,stroke:#333,stroke-width:2px;
    style D fill:#C0F6C0,stroke:#333,stroke-width:2px;
    style E fill:#C0F6C0,stroke:#333,stroke-width:2px;
    style F fill:#C0F6C0,stroke:#333,stroke-width:2px;
    style Z fill:#D4E6F1,stroke:#333,stroke-width:2px;

    style G fill:#F6C0C0,stroke:#333,stroke-width:2px;
    style H fill:#F6C0C0,stroke:#333,stroke-width:2px;
    style I fill:#F6C0C0,stroke:#333,stroke-width:2px;
    style J fill:#F6C0C0,stroke:#333,stroke-width:2px;
    style K fill:#F6C0C0,stroke:#333,stroke-width:2px;
```
### 2.1. Arquitectura L√≥gica

1. **Orquestador/Enrutador:**  
   - Recibe cada consulta del cliente.  
   - Usa un modelo ligero (ej: *GPT-4o Mini*) afinado para clasificaci√≥n.  
   - Decide: ¬øEs repetitiva (80%) o compleja (20%)?  

2. **Ruta del 80% (Eficiencia):**  
   - La consulta pasa al **Modelo Fine-Tuned LLM**.
   - Este modelo integra una capa de **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** para interactuar con la Base de Datos de EcoMarket (solo lectura de datos est√°ticos y f√°cticos, ej. estados de env√≠o, descripci√≥n de productos, devoluciones).

3. **Ruta del 20% (Capacidad):**
   - Si es compleja, la consulta pasa al **Modelo LLM**.
   - Este modelo usa la funcionalidad de Llamada a Funciones **(Function Calling)** para iniciar procesos especializados (ej: registrar una queja en el CRM o generar un ticket de soporte).
   - Si el modelo determina que la pregunta supera su capacidad de resoluci√≥n (ej. requiere una negociaci√≥n con el cliente), lo env√≠a a un Agente de Call Center, acompa√±ado de un **resumen** y un **sentiment score**.
   
---

### 2.2. Integraci√≥n con la Base de Datos (BD) de EcoMarket

| Mecanismo de Integraci√≥n | Prop√≥sito | Aplicaci√≥n en EcoMarket |
|--------------------------|-----------|--------------------------|
| **RAG (Retrieve Augmented Generation)** | Consultar datos en tiempo real (solo lectura). | Estado del pedido, informaci√≥n de env√≠o, inventario y cat√°logo. |
| **Function Calling / Herramientas** | Ejecutar acciones o registrar datos (lectura y escritura). | Registrar una devoluci√≥n, actualizar el status de una queja, o ejecutar un script de diagn√≥stico para un problema t√©cnico, ej.: Pedido no registrado, falla en acceso al portal de Ecomarket, etc. |

---

## 3. Justificaci√≥n

| Criterio               | Justificaci√≥n |
|-------------------------|---------------|
| **Costo** | Optimizaci√≥n del TCO. El 80% del tr√°fico va a un LLM Fine-Tuned econ√≥mico (ej. Mistral 7B). Solo se paga un modelo premium para el 20% de casos cr√≠ticos. |
| **Escalabilidad** | Escalabilidad paralela: el 80% se maneja con clusters de modelos ligeros en GPUs econ√≥micas. El 20% escala en la nube de proveedores de LLM de alto nivel (OpenAI/Google). |
| **Facilidad de Integraci√≥n** | Uso de est√°ndares como RAG y Function Calling, lo que simplifica el desarrollo y reduce la curva de aprendizaje del equipo de ingenier√≠a. |
| **Calidad de Respuesta** | Precisi√≥n garantizada en el 80% repetitivo (Fine-Tuning + RAG) y razonamiento superior en el 20% complejo (uso modelos avanzado). |

---

# Fase II - Evaluaci√≥n de Fortalezas, Limitaciones y Riesgos √âticos

### 4. Fortalezas
- **Reducci√≥n en los Tiempos de Respuesta de Servicio al Cliente:**  
  Es definitivamente el principal beneficio. El 80% de las consultas se resuelven en minutos, no en horas, impactando de manera ben√©fica la m√©trica **Satisfacci√≥n del Cliente**.

- **Disponibilidad 24/7:**  
  Por ser un modelo "Fine Tuned LLM", estar√° disponible de manera continua, 7x24, para manejar el alto volumen de consultas repetitivas. Esto de nuevo, redunda de manera positiva en la **Satisfacci√≥n al Cliente**.

- **Coherencia y Consistencia:**  
  Se asegura una √∫nica fuente de informaci√≥n para el 80% de las preguntas, eliminando las variaciones y errores que surgen de la respuesta humana. En otras palabras, reducci√≥n en las respuestas err√≥neas, inconsistentes o demoras en las respuestas.

- **Especializaci√≥n de la Labor del Agente Humano:**  
  Los agentes se liberan del trabajo repetitivo, pudiendo enfocarse solo en el 20% de los casos complejos, donde su empat√≠a y pensamiento cr√≠tico son realmente valiosos.

---

### 5. Limitaciones
- **Limitaci√≥n en la Empat√≠a:**  
  Si bien se ofrece un "fine tuned LLM" al igual que un Modelo LLM para responder a lo repetitivo y lo complejo. En los casos donde se requiere negociaci√≥n o manejo de crisis, estos modelos no pueden replicar la inteligencia emocional de un agente humano. Es por eso, que el Agente de Call Center debe atender al cliente en estas situaciones.

- **Dependencia en la Calidad de los Datos:**  
  Si queremos contar con una precisi√≥n mayor o igual al 80%, dependeremos completamente de la exactitud y limpieza de la **Base de Datos de EcoMarket**.  
  Si hay errores, informaci√≥n incompleta, muy pocos datos o inventados, el **LLM Fine-Tuned** generar√° respuestas erroneas o equivocadas. Esto afectara la **Satisfacci√≥n al Cliente** y pondr√° en riesgo la reputaci√≥n de la compa√±ia. Es lo siempre se maneja como lema con la Inteligencia Artificial: **Basura (Garbage)** genera **Basura (Garbage)**

- **Costos de Entrenamiento (Fine-Tuning):**  
  La inversi√≥n inicial en tiempo y recursos para afinar y hostear el modelo y genere una precisi√≥n mayor o igual al 80% puede ser significativa.Pero creemos que ser√° menor a usar un modelo LLM que tiene costos de Tokens y llamadas API. Aqu√≠ sera importante, planear adecuadamente para que los costos no sean altos y echen por tierra este proyecto.

---

### 6. Riesgos √âticos y de Gobernanza de Datos
- **Alucinaciones:**  
  Siempre existe el riesgo de que LM  invente informaci√≥n (una *"alucinaci√≥n"*) al generar datos de pedidos, productos, un resumen o un diagn√≥stico complejo.  
  *Mitigaci√≥n:* Implementar filtros de verificaci√≥n de hechos en la fase de **Function Calling** antes de generar la respuesta al cliente y mecanismo de validaci√≥n de respuestas a las preguntas repetitivas.

- **Sesgo:**  
  Si los datos utilizados para el Fine-Tuning contienen sesgos hist√≥ricos (ej. si las quejas de un grupo demogr√°fico espec√≠fico fueron mal manejadas hist√≥ricamente), el modelo podr√≠a perpetuar y automatizar ese trato preferencial o injusto.  
  *Mitigaci√≥n:* Auditor√≠a de sesgos en el dataset de entrenamiento y en las respuestas generadas.

- **Privacidad de Datos:**  
  Al integrar la BD de clientes (direcciones, historial de compras, entre otros) en el contexto de los prompts (ya sea **RAG** o **Function Calling**), se debe asegurar que:  
  1. Los datos est√°n anonimizados o se accede solo a la informaci√≥n m√≠nima necesaria.  
  2. Se implementa un estricto control de acceso y almacenamiento temporal del contexto (*borrado inmediato despu√©s de la respuesta*).
  3. Autenticaci√≥n de usuarios para identificar qui√©n accede y tener una auditor√≠a de todo el flujo de interacci√≥n.
  4. Usar un "Servicio Intermedio" para evitar que alguno de los modelos LLM usados interactu√© directamente con la o las bases de datos de **EcoMarket**

- **Impacto Laboral:**
  Los cambios Tecnologicos o de algun otro tipo en las compa√±ias siempre generan incertidumbre en la fuerza laboral activa. Tenemos clara que habr√° reducci√≥n del personal actual de Agentes usado para el servicio al cliente. Debemos manter como objetivo claro: que se trata de **mejorar, empoderar y no reemplazar**. La propuesta debe ser vista por EcoMarket como una herramienta que eleva el trabajo de los agentes de soporte, permiti√©ndoles enfocarse en la calidad y la retenci√≥n del cliente, en lugar de en la cantidad de tickets repetitivos que son lo que a hoy esta desbordado, haciendolo ineficiente y alta tasa de insatisfacci√≥n de los clientes; cr√≠tico para una compa√±ia emergente como EcoMarket.

  # Fase III - Aplicaci√≥n de Ingenier√≠a de Prompts

  A continuaci√≥n presentamos los propmts que hemos dise√±ado para los dos (2) requerimientos que se solicitan.

  ### 1. Estado de Solicitud de Pedido

  prompt_pedido = f"""
Eres un agente de servicio al cliente amable, profesional y conciso.
Tu √∫nica tarea es proporcionar el estado del pedido {num_pedido} con un **lenguaje natural y conversacional**, bas√°ndote estrictamente en la siguiente informaci√≥n:

- **N√∫mero de Pedido:** {num_pedido}
- **Estado Actual:** {estado}
- **Fecha de Entrega Estimada:** {estimacion_formateada}
- **Retrasado:** {retrasado}
{instruccion_tracking_data}

**INSTRUCCIONES DE FORMATO DE RESPUESTA:**
1.  Comienza con una frase que indique el estado y la estimaci√≥n de entrega (si aplica), usando el formato: "Su orden {num_pedido} se encuentra {frase_estado}. La fecha de entrega estimada es {estimacion_formateada}."
2.  Si la estimaci√≥n es 'No aplica' (como en el caso de 'Cancelado' o 'Entregado'), omite la parte de la fecha de entrega, solo indica el estado.
{instruccion_tracking}4. Si **Retrasado** es True, a√±ade el siguiente mensaje al final: "Nos excusamos por la demora en la entrega y estamos trabajando para que pueda contar con su orden lo mas pronto posible."
5.  Si el **Estado Actual** es 'Cancelado', a√±ade el siguiente mensaje al final: "Lamentamos los inconvenientes y le invitamos a comunicarse con nuestro centro de servicios al Nro. 01-800-XXX-XXXX para tener m√°s detalles."
6.  **IMPORTANTE:** Tu respuesta NO debe contener encabezados, listas numeradas, ni repetir la informaci√≥n de entrada, solo debe generar el texto conversacional.

Respuesta:
"""

  ### 2. Devoluci√≥n de Producto

     prompt = f"""
Act√∫a como un agente de servicio al cliente amable y profesional.
Usa la siguiente base de datos de productos:

{base_datos_devoluciones}

Instrucciones para el Asistente:
1. Responde inmediatamente al cliente siguiendo las instrucciones de formato.
2. Identifica el producto por su ID (ej. Producto 2002) y su nombre.
3. Revisa si el producto es retornable.
4. Si es retornable, indica **Retornable** y explica el proceso.
5. Si NO es retornable, indica **No retornable**, explica la raz√≥n y ofrece alternativas/descuentos.
6. Da la respuesta en el siguiente formato:
    - **Estado del producto**: (Retornable / No retornable)
    - **Explicaci√≥n**
    - **Siguientes pasos / Alternativas**
7. Mant√©n la respuesta clara y concisa, m√°ximo 5 p√°rrafos.
8. Usa un tono c√°lido, humano y profesional.

---
Cliente: Hola, estoy devolviendo {producto_buscado} por {motivo}, ¬øqu√© debo hacer?

{MARCADOR_ASISTENTE}"""

### 3. Prueba de los PROMPTS

Nosotros creamos un Google Colab Notebook que se llama IAG_Taller1_Fase_3, donde hicimos pruebas de los dos (2) Prompts dise√±ados. Este Google Colab Notebook esta en este repositorio de GitHUb y puede ser probado dando "Clic" con el rat√≥n en el icono "Open Google Colab".
