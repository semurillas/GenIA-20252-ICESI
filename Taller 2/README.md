<h1> <img width="207" height="112" alt="image" src="https://github.com/user-attachments/assets/89fd906b-04fb-4d4f-b5e6-8375083a8a01" /></h1>
<h1>üìö Maestr√≠a en Inteligencia Artificial Aplicada ‚Äì 3er Semestre</h1>

<h3>Asignatura: Inteligencia Artificial Generativa</h3>

<h3>Taller Pr√°ctico Nro. 2 </h3>

<hr style="width:60%;">

<h3>üë®‚Äçüéì Estudiantes</h3>
<ul style="list-style:none; padding:0; font-size:18px;">
    <li>Sebasti√°n Murillas</li>
    <li>Octavio Guerra</li>
</ul>

<hr style="width:60%;">

<h3>üìÖ Fecha: Octubre 13, 2025</h3>

## üìÇ Estructura del Repositorio

| Archivo / Carpeta | Tipo | Descripci√≥n |
|--------------------|------|-------------|
| `Taller 2/Documentos/Manual_de_Uso_Productos_Ecologicos.pdf` | PDF | Documento ejemplo de Manuales de Uso de Productos de la compa√±ia EcoMarket |
| `Taller 2/Documentos/Politica_de_Devoluciones_EcoMarket.pdf` | PDF | Documento ejemplo de Pol√≠ticas de Devoluciones de la compa√±ia EcoMarket |
| `Taller 2/Documentos/Terminos_y_Condiciones_Generales_de_Venta_EcoMarket.pdf` | PDF | Documento ejemplo de T√©rminos y Condiciones Generales de Venta de la compa√±ia EcoMarket |
| `Taller 2/Documentos/faq_ecomarket.json` | JSON | Documento ejemplo de Preguntas Frecuentes recopiladas para la compa√±ia EcoMarket |
| `Taller 2/Documentos/inventario_productos_ecomarket.csv` | CSV | Documento ejemplo de Inventario de Productos de la compa√±ia EcoMarket |
| `Taller 2/Documentos/pedidos_ecomarket.csv` | CSV | Documento ejemplo de Estados de Pedidos de la compa√±ia EcoMarket |
| `Taller 2/Documentos/IAG_Taller_2_Fase3.ipynb` | Notebook (Colab / Jupyter) | Google Colab Notebook desarrollado para la Fase 3 de este Taller |
| `Taller 2/Documentos/README.md` | Markdown | Este archivo explicativo, con las fases I, II y III del Taller Nro. 2 |



# üß† Fase I: Selecci√≥n de Componentes Clave del Sistema RAG para EcoMarket

---

## üèóÔ∏è Contexto del Proyecto

**EcoMarket**, una empresa dedicada a la venta de productos sostenibles y en pleno proceso de crecimiento, ha enfrentado recientemente cuellos de botella en su servicio de atenci√≥n al cliente. En algunos casos, los tiempos de respuesta han llegado hasta las 24 horas, lo que ha incrementado el √≠ndice de insatisfacci√≥n de los usuarios.

Nosotros, como especialistas en Inteligencia Artificial Generativa, hemos iniciado una asistencia t√©cnica para abordar este problema. En la primera fase realizada, propusimos el uso de Modelos de Lenguaje Extenso (LLM), Modelos de Embeddings y Bases de Datos Vectoriales, con el objetivo de reducir los tiempos de respuesta y, al mismo tiempo, mejorar la satisfacci√≥n del cliente.

Los Large Language Models (LLM), si bien son potentes y de prop√≥sito general, no cuentan con conocimiento espec√≠fico sobre la informaci√≥n interna de una empresa. Por esta raz√≥n, como equipo asesor, planteamos la implementaci√≥n de un sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG, Retrieval-Augmented Generation). Este enfoque permitir√° que el LLM responda preguntas precisas bas√°ndose en documentos reales de la organizaci√≥n ‚Äîcomo descripciones de productos, pol√≠ticas internas o lineamientos de sostenibilidad‚Äî, reduciendo errores y evitando respuestas no fundamentadas (alucinaciones).

En esta etapa, como parte del equipo que asiste a EcoMarket, debemos seleccionar los dos componentes fundamentales del sistema RAG:

1. **El modelo de embeddings**, cuya elecci√≥n depender√° de su precisi√≥n, costo y capacidad para manejar el idioma espa√±ol.

2. **La base de datos vectorial**, que debe ofrecer eficiencia en las b√∫squedas, buena escalabilidad y facilidad de integraci√≥n con el sistema.
---

## üéØ Objetivo de la Fase I

Antes de iniciar la implementaci√≥n, es necesario definir los dos componentes principales del sistema RAG:

1. **Modelo de Embeddings:** encargado de transformar el texto en vectores num√©ricos que representen su significado sem√°ntico.  
2. **Base de Datos Vectorial:** responsable de almacenar y recuperar esos vectores de manera eficiente, facilitando la b√∫squeda por similitud.

---

## üß© 1. Decisi√≥n del Modelo de Embeddings

El **modelo de embeddings** convierte los textos de EcoMarket en **representaciones vectoriales** que capturan su significado.  
Para la empresa, es fundamental lograr **alta precisi√≥n sem√°ntica en espa√±ol**, mantener **bajo costo operativo**, y asegurar la **posibilidad de escalamiento futuro**.

---

### 1.1. Opciones Evaluadas y Justificaci√≥n

| **Modelo de Embedding** | **Tipo** | **Precisi√≥n en Espa√±ol (RAG)** | **Costo por Uso** | **Limitaciones** | **Justificaci√≥n de la Elecci√≥n** |
|--------------------------|----------|---------------------------------|-------------------|------------------|----------------------------------|
| **BGE-M3 (BAAI General Embedding)** | C√≥digo Abierto (Hugging Face) | Alta. Excelente rendimiento multiling√ºe, optimizado para espa√±ol. Soporta contexto largo (8192 tokens). | Costo cero (requiere infraestructura propia). | Necesita m√°s recursos de c√≥mputo (GPU/CPU). | Ofrece el mejor equilibrio entre precisi√≥n sem√°ntica, soporte multiling√ºe y control sobre los datos. Ideal para una empresa en desarrollo que busca evitar costos por API y mantener soberan√≠a tecnol√≥gica. |
| **Multilingual E5-base** | C√≥digo Abierto (Hugging Face) | Buena. Ligero y eficiente, con resultados s√≥lidos en espa√±ol. | Costo cero. Muy eficiente en CPU. | Rendimiento algo inferior en recuperaci√≥n sem√°ntica compleja. | Alternativa viable si se prioriza velocidad o disponibilidad de recursos limitados. |
| **OpenAI text-embedding-3-small** | Propietario (API de pago) | Muy alta. Resultados excelentes en espa√±ol y gran consistencia vectorial. | Pago por cada texto procesado. | Dependencia de un proveedor externo. Costos acumulativos en grandes vol√∫menes. | Apropiado para entornos empresariales maduros con presupuesto para servicios en la nube. No recomendado en esta etapa inicial. |

---

### 1.2. üèÜ Modelo Seleccionado: **BGE-M3 (BAAI General Embedding)**

La propuesta de nuestro equipo es utilizar el modelo **BGE-M3**, de c√≥digo abierto, desarrollado por **BAAI**.  

#### **Justificaci√≥n t√©cnica:**
- **Alto rendimiento** en tareas de b√∫squeda sem√°ntica en espa√±ol.  
- **Soporte multiling√ºe**, ideal si EcoMarket expande operaciones a otros mercados.  
- **Costo nulo de licencia**, al ejecutarse localmente o en entornos cloud controlados por la empresa.  
- **Evita dependencia** de servicios externos, favoreciendo la privacidad de los datos internos.  

Esta elecci√≥n permite a EcoMarket iniciar con un modelo potente y gratuito, con capacidad de escalar posteriormente a soluciones propietarias si se requiere mayor rendimiento o soporte.

---

## üóÉÔ∏è 2. Decisi√≥n de la Base de Datos Vectorial

La **Base de Datos Vectorial** es el componente que almacenar√° los embeddings generados y permitir√° realizar b√∫squedas por similitud sem√°ntica.  
Para EcoMarket, los criterios clave son la **escalabilidad**, el **costo operativo** y la **facilidad de integraci√≥n** con herramientas de desarrollo modernas (como LangChain o LlamaIndex).

---

### 2.1. Opciones Evaluadas y Justificaci√≥n

| **Base de Datos** | **Tipo** | **Escalabilidad** | **Facilidad de Uso** | **Ventajas para EcoMarket** | **Desventajas** |
|-------------------|----------|-------------------|----------------------|-----------------------------|-----------------|
| **ChromaDB** | Vectorial pura / Open Source | Media (ideal para entornos de desarrollo o medianos vol√∫menes de datos). | Muy alta. Instalaci√≥n simple y gran integraci√≥n con frameworks de IA. | Costo cero. R√°pida implementaci√≥n local o cloud. Perfecta para validaci√≥n de concepto o entornos en crecimiento. | No est√° dise√±ada para cargas de producci√≥n masivas. |
| **Qdrant** | Vectorial pura / Open Source | Alta. Optimizada para grandes vol√∫menes y rendimiento en producci√≥n. | Media. Requiere despliegue en contenedor (Docker o Kubernetes). | Permite b√∫squedas vectoriales y filtrado avanzado por metadatos (ej. ‚Äúproductos sostenibles‚Äù). | Configuraci√≥n inicial m√°s compleja. |
| **pgvector (PostgreSQL Extension)** | Extensi√≥n de base de datos SQL / Open Source | Media-Alta. Escalable dentro del ecosistema PostgreSQL. | Alta. Ideal si la empresa ya usa PostgreSQL. | Permite integrar datos vectoriales y estructurados en un mismo entorno. Cumple con ACID. | No es una base vectorial pura; menor rendimiento frente a Qdrant o Pinecone en escalas grandes. |
| **Pinecone** | Propietario (SaaS) | Muy alta. Totalmente administrada en la nube. | Alta. Integraci√≥n sencilla por API. | Escalabilidad inmediata y soporte empresarial. | Costos por volumen de datos y dependencia total de un proveedor externo. |

---

### 2.2. üèÜ Base de Datos Seleccionada: **ChromaDB**

Proponemos **ChromaDB** como base de datos vectorial inicial para EcoMarket.

#### **Justificaci√≥n t√©cnica y estrat√©gica:**
- **Simplicidad de implementaci√≥n:** se integra f√°cilmente con pipelines de RAG usando Python y librer√≠as como LangChain.  
- **Costo cero:** al ser open source, no implica gastos de licencia o suscripci√≥n.  
- **Ideal para un entorno en desarrollo:** permite concentrar esfuerzos en la optimizaci√≥n del flujo de embeddings y recuperaci√≥n antes de escalar.  
- **Compatibilidad con diferentes modelos:** se adapta f√°cilmente a cambios futuros de modelo de embeddings o infraestructura.
  
---
## üß† 3. Arquitectura RAG Propuesta

El sistema RAG propuesto para EcoMarket seguir√° la siguiente arquitectura base:

```text
Documentos de EcoMarket
        ‚îÇ
        ‚ñº
   BGE-M3 (Embeddings)
        ‚îÇ
        ‚ñº
Vectores Num√©ricos
        ‚îÇ
        ‚ñº
   ChromaDB (Almacenamiento y B√∫squeda)
        ‚îÇ
        ‚ñº
   LLM (Generaci√≥n de Respuestas)

```
---

# üß† Fase II: Construcci√≥n de la Base de Conocimiento (Indexaci√≥n y Segmentaci√≥n)

En esta fase se construye la base de conocimiento del sistema RAG (Retrieval-Augmented Generation). El proceso consiste en segmentar los documentos procesados y almacenarlos en una base de datos vectorial utilizando un modelo de embeddings especializado. Esta etapa es fundamental para que el sistema pueda recuperar informaci√≥n relevante de manera sem√°ntica ante una consulta del usuario.

## 1. Segmentaci√≥n de los documentos

Previo a la indexaci√≥n, los textos son divididos en fragmentos manejables con el objetivo de optimizar la recuperaci√≥n sem√°ntica. La segmentaci√≥n se realiza empleando un Text Splitter, el cual corta los documentos en bloques con una longitud m√°xima controlada (por ejemplo, 1.000 caracteres) y un solapamiento entre ellos (por ejemplo, 200 caracteres). Este solapamiento permite preservar el contexto entre fragmentos contiguos y evita p√©rdida de informaci√≥n en los l√≠mites de los textos.

Cada fragmento resultante mantiene una relaci√≥n directa con el documento original, asegurando trazabilidad y precisi√≥n durante el proceso de recuperaci√≥n posterior. El resultado de esta etapa son los documentos segmentados (final_docs), que sirven como insumo para la generaci√≥n de embeddings.

## 2. Generaci√≥n de embeddings con BGE-M3

Una vez segmentados los textos, se generan sus representaciones vectoriales mediante el modelo BGE-M3, alojado en Hugging Face. Este modelo convierte los fragmentos de texto en vectores de alta dimensi√≥n que capturan el significado sem√°ntico de cada fragmento.

El modelo se inicializa de la siguiente forma:

model_name: "BAAI/bge-m3"

model_kwargs: define que la ejecuci√≥n se realice en GPU ('device': 'cuda') para aprovechar la aceleraci√≥n de c√≥mputo.

encode_kwargs: incluye la normalizaci√≥n de los embeddings ('normalize_embeddings': True), lo que mejora la coherencia en las comparaciones vectoriales.

Este proceso garantiza que los embeddings mantengan relaciones espaciales consistentes, permitiendo medir la similitud entre consultas y documentos.

## 3. Indexaci√≥n en la base de datos vectorial (ChromaDB)

Con los embeddings generados, se construye la base de datos vectorial utilizando ChromaDB. Esta herramienta almacena los vectores en un formato optimizado para b√∫squedas de similitud, lo cual permite recuperar los documentos m√°s relevantes en funci√≥n del significado de la consulta, no solo de coincidencias l√©xicas.

Durante la creaci√≥n de la base vectorial:

Se asigna un nombre a la colecci√≥n (collection_name="ecomarket_rag_data") que identifica el conjunto de embeddings.

Se define un directorio local para persistir los datos (persist_directory="./chroma_db"), lo que permite reutilizar la base sin recalcular los embeddings en ejecuciones futuras.

A partir de la base Chroma creada, se construye un retriever, encargado de buscar los fragmentos m√°s cercanos sem√°nticamente a la consulta del usuario. En este caso, se define que recupere los tres documentos m√°s relevantes (k=3) mediante una b√∫squeda basada en similitud de coseno.

## 4. Resultado final

Al finalizar la fase, la base vectorial queda configurada y lista para integrarse en el flujo del modelo RAG. Esta estructura permite que, ante una pregunta, el sistema recupere los fragmentos m√°s sem√°nticamente relacionados y los use como contexto para generar una respuesta precisa y fundamentada.

# üß† Fase III: Integraci√≥n y Ejecuci√≥n del C√≥digo

Para esta fase hemos desarrollado el **Google Colab Notebook** con el nombre `IAG_Taller_2_Fase3.ipynb`, donde implementamos el **modelo LLM Mistral-7B Instruct** ([Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)), el mismo utilizado en el **Taller Pr√°ctico N.¬∫ 1**.  
En este notebook construimos adem√°s un **modelo RAG (Retrieval-Augmented Generation)** empleando como **modelo de embeddings** `BAAI/bge-m3` y como **base de datos vectorial** `ChromaDB` y usando documentos que hemos creado que nos permiten que las busquedas se hagan en informaci√≥n propia de la compa√±ia, permitiendo asi que sea el contexto usado por el Modelo LLM al responder a las preguntas que se simulan con usuarios o clientes.  

El archivo puede ejecutarse directamente en **Google Colab**, configurando el entorno de ejecuci√≥n con **T4 GPU** y **High RAM**.  
Al ejecutar las celdas en orden, se podr√° observar:  
- La carga de librer√≠as y m√≥dulos requeridos.  
- La construcci√≥n del modelo RAG con `BAAI/bge-m3` y `ChromaDB` con seis (6) documentos: Inventario de Productos (Formato CSV), Estados de Pedidos (Formato CSV), Pol√≠ticas de Devoluci√≥n de EcoMarket (Formato PDF), Terminos Generales de Ventas de EcoMarket (Formato PDF), Manuales de Usuario de Productos (Formato PDF), y Preguntas Frecuentes (Formato JSON).
- La carga del modelo LLM **Mistral-7B** en modo 4-bit.  
- La generaci√≥n de *prompts* y respuestas basadas en el modelo RAG, presentadas en el **Paso N.¬∫ 11**, donde se muestran los resultados obtenidos frente a las preguntas de prueba.
