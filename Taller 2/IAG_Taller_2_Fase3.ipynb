{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>\ud83d\udcda Maestr\u00eda en Inteligencia Artificial Aplicada \u2013 3er Semestre</h1>\n",
    "\n",
    "<h3>Asignatura: Inteligencia Artificial Generativa</h3>\n",
    "\n",
    "<h4>Taller Pr\u00e1ctico Nro. 2 </h4>\n",
    "\n",
    "\n",
    "<hr style=\"width:60%;\">\n",
    "\n",
    "<h2>\ud83d\udc68\u200d\ud83c\udf93 Estudiantes</h2>\n",
    "<ul style=\"list-style:none; padding:0; font-size:18px;\">\n",
    "    <li>Sebasti\u00e1n Murillas</li>\n",
    "    <li>Octavio Guerra</li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"width:60%;\">\n",
    "\n",
    "<h3>\ud83d\udcc5 Fecha: Octubre 13, 2025</h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/semurillas/GenIA-20252-ICESI/blob/main/Taller%202/IAG_Taller2_Fase_3.ipynb)"
   ],
   "metadata": {
    "id": "Szxd6ROE-hXq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. Instalaci\u00f3n de librer\u00edas Requeridas**\n",
    "\n",
    "En la siguiente celda de c\u00f3digo instalamos las principales librer\u00edas necesarias para construir nuestro entorno de Retrieval-Augmented Generation (RAG). Estas incluyen herramientas para el manejo de datos, carga de documentos, uso de modelos de lenguaje (LLM), el Modelo de embeddings que usaremos: BGE-M3, as\u00ed como la base de datos vectorial que usaremos: ChromaDB. Adicional, tambi\u00e9n se instalan librer\u00edas que optimizan el rendimiento de los modelos, permitiendo su ejecuci\u00f3n eficiente."
   ],
   "metadata": {
    "id": "ualdD06g_zuK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZDwpPsl-a7c",
    "outputId": "4405feed-3a68-46c9-bcd3-9e4c8940e1f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Importaciones b\u00e1sicas del sistema\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Instalaci\u00f3n de librer\u00edas necesarias para el entorno RAG\n",
    "!pip install -U langchain-community langchain-huggingface sentence-transformers chromadb jq -q\n",
    "\n",
    "# Instalamos librer\u00edas para manejar diferentes formatos de archivo\n",
    "!pip install pypdf pandas unstructured -q\n",
    "\n",
    "# Instalamos la librer\u00eda Transformers (para usar modelos de Hugging Face)\n",
    "# y Accelerate (para optimizar la ejecuci\u00f3n en CPU/GPU).\n",
    "!pip install transformers accelerate -q\n",
    "\n",
    "# Instalamos BitsAndBytes, que nos permite cargar modelos grandes\n",
    "# usando cuantizaci\u00f3n para reducir el consumo de memoria.\n",
    "!pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. Importaci\u00f3n de librer\u00edas Requeridas**\n",
    "\n",
    "Importaci\u00f3n de las Librer\u00edas requeridas para la construcci\u00f3n del nuestro entorno de Retrieval-Augmented Generation (RAG), en el que usaremos el Modelo de Embeddings GE-M3 y la base de datos vectorial que usaremos: ChromaDB"
   ],
   "metadata": {
    "id": "TnIZfu2Q_5bq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Importamos la librer\u00eda requests para realizar descargas o peticiones HTTP.\n",
    "import requests\n",
    "\n",
    "# Importamos la librer\u00eda os para manejar rutas y archivos del sistema operativo.\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Importaciones de Modulos de LangChain para el entorno RAG\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Importamos Chroma, la base de datos vectorial usada para almacenar y buscar embeddings.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Importamos el modelo de embeddings BGE desde Hugging Face, usado para representar textos num\u00e9ricamente.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Importamos la clase Document, que define la estructura b\u00e1sica de los documentos cargados.\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Importamos RunnablePassthrough, que permite encadenar pasos en un flujo RAG sin modificar los datos.\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Importamos StrOutputParser, que convierte la salida del modelo en texto legible.\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# Importamos PromptTemplate, que define las plantillas de los prompts que se enviar\u00e1n al modelo LLM.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Importaciones de Modulos para carga y procesamiento de documentos\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Importamos los loaders que permiten leer archivos PDF, CSV y JSON desde LangChain Community.\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, JSONLoader\n",
    "\n",
    "# Importamos el divisor de texto recursivo, que separa los documentos en fragmentos manejables.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Importaciones de Modulos para el modelo de lenguaje (LLM)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Importamos AutoTokenizer, AutoModelForCausalLM, pipeline y BitsandBytesConfig desde Transformers,\n",
    "# para cargar y ejecutar modelos de lenguaje de Hugging Face reduciendo el uso de Memoria y optimizando\n",
    "# velocidad en procesamiento sin que se pierda mucha precisi\u00f3n.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "# Importa PyTorch para manejar tensores y tipos de datos\n",
    "import torch"
   ],
   "metadata": {
    "id": "T9DjJlljABie"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **3. Acceso a Hugging Faces**\n",
    "\n",
    "Procedemos a realizar el login en Hugging Face Hub usando un token de acceso. Esto es necesario para poder descargar y cargar modelos LLM privados o de gran tama\u00f1o. En nuestro caso, este paso asegura el acceso al modelo Mistral-7B-Instruct-v0.2, que usaremos m\u00e1s adelante utilizaremos para la generacion de las respuestas para solicitudes de pedidos y asistenci en solicitudes o preguntas referentes a devoluciones."
   ],
   "metadata": {
    "id": "52N0AXqpAJol"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Logueo en Hugging Faces para que pueda ser posible hacer uso\n",
    "# del LLM seleccionado para la construcci\u00f3n de los PROMPTS que daran\n",
    "# respuestas a solicitudes de estados de Pedidos y Asistencia al Proceso de\n",
    "# Devoluci\u00f3n de un Producto.\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ],
   "metadata": {
    "id": "D6RnBIgkAOIJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **4. Descarga de Archivos desde GitHub**\n",
    "\n",
    "En la siguiente celda de c\u00f3digo en Python se define una funci\u00f3n que permite descargar archivos directamente desde un repositorio de GitHub utilizando una URL del contenido crudo (raw content). Esta funci\u00f3n resulta \u00fatil en el proceso de construcci\u00f3n de un sistema RAG (Retrieval-Augmented Generation), ya que posibilita obtener y almacenar localmente los archivos que servir\u00e1n como fuente de conocimiento.\n",
    "Posteriormente, estos archivos podr\u00e1n ser integrados en el modelo LLM seleccionado, permitiendo que el sistema genere respuestas basadas en informaci\u00f3n actualizada y contextual, como por ejemplo, consultas de clientes sobre el estado de pedidos o solicitudes de devoluci\u00f3n.\n"
   ],
   "metadata": {
    "id": "LGxvr3PdpdOI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcion para descargar un archivo de la URL de \"raw\" de GitHub y\n",
    "# salvarlo localmente para el Google Colab Notebook.\n",
    "def download_file_from_github(raw_url: str, local_filename: str) -> str:\n",
    "\n",
    "    # Aseguramos el uso de la ruta est\u00e1ndar 'main'\n",
    "    corrected_url = raw_url.replace(\"/refs/heads/main/\", \"/main/\")\n",
    "\n",
    "    # Muestra en pantalla el inicio de descarga del Archivo del URL raw de GitHub\n",
    "    print(f\"Descargando {local_filename}...\")\n",
    "    try:\n",
    "        # Descarga el contenido del archivo\n",
    "        response = requests.get(corrected_url, stream=True)\n",
    "        response.raise_for_status() # Lanza una excepci\u00f3n si hay un error HTTP\n",
    "\n",
    "        # Guardar el archivo localmente en el directorio de Colab\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        # Confirma en pantalla que el archivo desde el URL raw de GitHub fue descargado\n",
    "        print(f\"Descarga exitosa. Guardado en: {local_filename}\")\n",
    "\n",
    "        # Retorna el nombre de archivo local con que fue guardadado en el directorio\n",
    "        # local de Google Colab.\n",
    "        return local_filename\n",
    "\n",
    "    # Si la descarga del archivo desde el URL raw de GitHub falla\n",
    "    # (e.g., archivo no encontrado, error 404), detiene el proceso\n",
    "    except requests.exceptions.RequestException as e:\n",
    "           raise RuntimeError(f\"ERROR FATAL: No se pudo descargar {local_filename} desde {corrected_url}. Verifique la URL y el nombre del archivo.\") from e"
   ],
   "metadata": {
    "id": "Ws3Zc842luWu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **5. Carga de Archivos para RAG**\n",
    "\n",
    "Desde un repositorio en GitHub, extraemos, descargamos y guardamos en local para el Notebook, los archivos que usaremos para luego llevar a nuestro Modelo de RAG. Estos archivos seran la fuente de informaci\u00f3n que estar\u00e1n en la base de datos vectorial y que servir\u00e1n para buscar respuestas a las solicitudes de estados de pedidos y asistencia en devoluci\u00f3n en preguntas hechas por clientes."
   ],
   "metadata": {
    "id": "6w6YUYnsAbCz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definimos la URL base del contenido \"raw\" del repositorio de GitHub,\n",
    "# desde donde descargaremos los archivos que alimentar\u00e1n nuestro modelo RAG.\n",
    "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/semurillas/GenIA-20252-ICESI/main/Taller%202/Documentos/\"\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4.1. Carga de archivos CSV de Pedidos e Inventario.\n",
    "# ----------------------------------------------------\n",
    "# En este bloque cargamos los datos tabulares (CSV) que servir\u00e1n como fuente estructurada\n",
    "# para responder preguntas relacionadas con pedidos e inventarios.\n",
    "\n",
    "# 4.1.1. Pedidos\n",
    "# Definimos el nombre del archivo CSV de pedidos que guardaremos localmente.\n",
    "local_path_pedidos = \"pedidos_ecomarket.csv\"\n",
    "\n",
    "# Construimos la URL completa del archivo sumando la ruta base y el nombre del archivo.\n",
    "pedidos_url = GITHUB_RAW_URL + local_path_pedidos\n",
    "\n",
    "# Descargamos el archivo desde GitHub y lo guardamos con el nombre definido localmente.\n",
    "download_file_from_github(pedidos_url, local_path_pedidos)\n",
    "\n",
    "# Cargamos el archivo CSV con el loader de LangChain, usando codificaci\u00f3n UTF-8.\n",
    "pedidos_loader = CSVLoader(file_path=local_path_pedidos, encoding=\"utf-8\")\n",
    "\n",
    "# Transformamos el archivo CSV en documentos que el modelo podr\u00e1 usar como conocimiento.\n",
    "pedidos_docs = pedidos_loader.load()\n",
    "\n",
    "# Mostramos la cantidad de documentos cargados desde el archivo de pedidos.\n",
    "print(f\"Documentos de Pedidos cargados localmente: {len(pedidos_docs)} \\n\")\n",
    "nro_files = 1\n",
    "\n",
    "# 4.1.2. Inventario\n",
    "# Definimos el nombre del archivo CSV con el inventario de productos.\n",
    "local_path_inventario = \"inventario_productos_ecomarket.csv\"\n",
    "\n",
    "# Construimos la URL completa del archivo de inventario.\n",
    "inventario_url = GITHUB_RAW_URL + local_path_inventario\n",
    "\n",
    "# Descargamos el archivo de inventario desde GitHub y lo guardamos localmente.\n",
    "download_file_from_github(inventario_url, local_path_inventario)\n",
    "\n",
    "# Cargamos el archivo CSV de inventario como documentos con LangChain.\n",
    "inventario_loader = CSVLoader(file_path=local_path_inventario, encoding=\"utf-8\")\n",
    "\n",
    "# Transformamos el contenido en documentos utilizables por el modelo.\n",
    "inventario_docs = inventario_loader.load()\n",
    "\n",
    "# Imprimimos la cantidad de documentos cargados del inventario.\n",
    "print(f\"Documentos de Inventario cargados localmente: {len(inventario_docs)}\\n\")\n",
    "nro_files +=1\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4.2. Carga de archivos PDF de Politicas de Devoluci\u00f3n,\n",
    "#      T\u00e9rminos y Condiciones generales de Ventas y Manual\n",
    "#      de uso de Productos.\n",
    "# ----------------------------------------------------\n",
    "# 4.2.1. Pol\u00edticas de Devoluci\u00f3n\n",
    "# Definimos el nombre local del archivo PDF de pol\u00edticas de devoluci\u00f3n.\n",
    "local_path_devoluciones = \"Politica_de_Devoluciones_EcoMarket.pdf\"\n",
    "\n",
    "# Construimos la URL del archivo PDF en el repositorio de GitHub.\n",
    "devoluciones_url = GITHUB_RAW_URL + \"Politica_de_Devoluciones_EcoMarket.pdf\"\n",
    "\n",
    "# Descargamos el archivo PDF desde GitHub.\n",
    "download_file_from_github(devoluciones_url, local_path_devoluciones)\n",
    "\n",
    "# Cargamos el archivo PDF con el loader de PyPDFLoader para procesar su contenido.\n",
    "devoluciones_loader = PyPDFLoader(local_path_devoluciones)\n",
    "\n",
    "# Dividimos el contenido del PDF en fragmentos (chunks) manejables por el modelo.\n",
    "devoluciones_docs = devoluciones_loader.load_and_split()\n",
    "\n",
    "# Mostramos la cantidad de fragmentos cargados del documento de devoluciones.\n",
    "print(f\"Documentos de Pol\u00edticas de Devoluci\u00f3n cargados localmente: {len(devoluciones_docs)}\\n\")\n",
    "nro_files +=1\n",
    "# 4.2.2. T\u00e9rminos y Condiciones\n",
    "# Definimos el nombre local del archivo PDF de t\u00e9rminos y condiciones.\n",
    "local_path_terminos = \"Terminos_y_Condiciones_EcoMarket.pdf\"\n",
    "\n",
    "# Construimos la URL del archivo original en GitHub.\n",
    "terminos_url = GITHUB_RAW_URL + \"Terminos_y_Condiciones_Generales_de_Venta_EcoMarket.pdf\"\n",
    "\n",
    "# Descargamos el PDF con los t\u00e9rminos y condiciones.\n",
    "download_file_from_github(terminos_url, local_path_terminos)\n",
    "\n",
    "# Cargamos el archivo PDF y lo procesamos con PyPDFLoader.\n",
    "terminos_loader = PyPDFLoader(local_path_terminos)\n",
    "\n",
    "# Dividimos su contenido en secciones legibles por el modelo.\n",
    "terminos_docs = terminos_loader.load_and_split()\n",
    "\n",
    "# Mostramos la cantidad de fragmentos procesados del documento.\n",
    "print(f\"Documentos de T\u00e9rminos y Condiciones cargados localmente: {len(terminos_docs)}\\n\")\n",
    "nro_files +=1\n",
    "\n",
    "# 4.2.3. Manual de Usuario\n",
    "# Definimos el nombre local del archivo PDF del manual de uso de productos.\n",
    "local_path_manual = \"Manual_de_Uso_Productos_Ecologicos.pdf\"\n",
    "\n",
    "# Construimos la URL del archivo en GitHub.\n",
    "manual_url = GITHUB_RAW_URL + \"Manual_de_Uso_Productos_Ecologicos.pdf\"\n",
    "\n",
    "# Descargamos el archivo PDF desde GitHub.\n",
    "download_file_from_github(manual_url, local_path_manual)\n",
    "\n",
    "# Cargamos el archivo con PyPDFLoader para extraer su texto.\n",
    "manual_loader = PyPDFLoader(local_path_manual)\n",
    "\n",
    "# Dividimos el contenido en fragmentos que el modelo podr\u00e1 procesar.\n",
    "manual_docs = manual_loader.load_and_split()\n",
    "\n",
    "# Mostramos la cantidad de fragmentos cargados del manual.\n",
    "print(f\"Documento de Manual de Uso de Producto cargado localmente: {len(manual_docs)}\\n\")\n",
    "nro_files +=1\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# C. Carga de archivo FAQ en formato JSON\n",
    "# ----------------------------------------------------\n",
    "# En este bloque cargamos el archivo JSON con preguntas frecuentes (FAQs)\n",
    "# que permitir\u00e1n al modelo responder consultas comunes de los usuarios.\n",
    "\n",
    "# Definimos el nombre local del archivo JSON de FAQs.\n",
    "local_path_json = \"faq_ecomarket.json\"\n",
    "\n",
    "# Construimos la URL del archivo JSON en el repositorio.\n",
    "json_url = GITHUB_RAW_URL + local_path_json\n",
    "\n",
    "# Descargamos el archivo JSON desde GitHub.\n",
    "download_file_from_github(json_url, local_path_json)\n",
    "\n",
    "# Cargamos el archivo JSON con el loader de LangChain, indicando su esquema.\n",
    "json_loader = JSONLoader(\n",
    "    file_path=local_path_json,  # Usamos la ruta local\n",
    "    jq_schema='.[]',            # Leemos cada elemento del array JSON\n",
    "    text_content=False,         # Indicamos que no todo es texto plano\n",
    "    json_lines=True             # Habilitamos lectura por l\u00edneas JSON\n",
    ")\n",
    "\n",
    "# Transformamos el JSON en documentos procesables por el modelo.\n",
    "faq_docs = json_loader.load()\n",
    "\n",
    "# Mostramos la cantidad de FAQs cargadas correctamente.\n",
    "print(f\"Documentos de FAQs cargados localmente: {len(faq_docs)}\\n\")\n",
    "nro_files +=1\n",
    "\n",
    "# Mostramos la cantidad de archivos\n",
    "print(f\"Total de archivos descargados y preparados para el Modelo RAG: {nro_files}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huqEVx-kAlgE",
    "outputId": "6f97ebf9-04dc-4722-931d-681adbbc1cf7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Descargando pedidos_ecomarket.csv...\n",
      "Descarga exitosa. Guardado en: pedidos_ecomarket.csv\n",
      "Documentos de Pedidos cargados localmente: 15 \n",
      "\n",
      "Descargando inventario_productos_ecomarket.csv...\n",
      "Descarga exitosa. Guardado en: inventario_productos_ecomarket.csv\n",
      "Documentos de Inventario cargados localmente: 24\n",
      "\n",
      "Descargando Politica_de_Devoluciones_EcoMarket.pdf...\n",
      "Descarga exitosa. Guardado en: Politica_de_Devoluciones_EcoMarket.pdf\n",
      "Documentos de Pol\u00edticas de Devoluci\u00f3n cargados localmente: 2\n",
      "\n",
      "Descargando Terminos_y_Condiciones_EcoMarket.pdf...\n",
      "Descarga exitosa. Guardado en: Terminos_y_Condiciones_EcoMarket.pdf\n",
      "Documentos de T\u00e9rminos y Condiciones cargados localmente: 3\n",
      "\n",
      "Descargando Manual_de_Uso_Productos_Ecologicos.pdf...\n",
      "Descarga exitosa. Guardado en: Manual_de_Uso_Productos_Ecologicos.pdf\n",
      "Documento de Manual de Uso de Producto cargado localmente: 10\n",
      "\n",
      "Descargando faq_ecomarket.json...\n",
      "Descarga exitosa. Guardado en: faq_ecomarket.json\n",
      "Documentos de FAQs cargados localmente: 250\n",
      "\n",
      "Total de archivos descargados y preparados para el Modelo RAG: 6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **6. Agrupaci\u00f3n y Segmentaci\u00f3n de Documentos para EL MODELO RAG**\n",
    "\n",
    "En la siguente celda de c\u00f3digo se agrupan y procesan los documentos que formar\u00e1n la base de conocimiento del sistema RAG.\n",
    "Los archivos estructurados (CSV y JSON) se cargan directamente, mientras que los PDFs se dividen en fragmentos para facilitar su comprensi\u00f3n por el modelo.\n",
    "Finalmente, se unifican todos los documentos en una sola colecci\u00f3n lista para ser indexada y utilizada por el modelo de lenguaje."
   ],
   "metadata": {
    "id": "O4X5il2gY85o"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Agrupamos los documentos que no requieren divisi\u00f3n, como los archivos CSV y JSON.\n",
    "docs_no_split = pedidos_docs + inventario_docs + faq_docs\n",
    "\n",
    "# Mostramos la cantidad total de documentos estructurados sin dividir.\n",
    "print(f\"Documentos Tabulares/Estructurados (sin dividir): {len(docs_no_split)}\")\n",
    "\n",
    "# Agrupamos los documentos que s\u00ed necesitan ser divididos, en este caso los archivos PDF.\n",
    "docs_to_split = devoluciones_docs + terminos_docs\n",
    "\n",
    "# Mostramos la cantidad total de documentos largos que ser\u00e1n fragmentados.\n",
    "print(f\"Documentos Largos (a dividir): {len(docs_to_split)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Segmetnaci\u00f3n de Texto solo para los PDFs\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Definimos el divisor de texto, con fragmentos de 1000 caracteres y un solapamiento de 200\n",
    "# para conservar coherencia contextual entre secciones.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Aplicamos la divisi\u00f3n a los documentos PDF para generar los fragmentos (chunks).\n",
    "pdf_chunks = text_splitter.split_documents(docs_to_split)\n",
    "\n",
    "# Mostramos la cantidad total de fragmentos generados desde los PDFs.\n",
    "print(f\"Total de chunks generados de los PDFs: {len(pdf_chunks)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Unificaci\u00f3n de los Chunks Finales (No segmentados y segmentados)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Unimos los documentos no divididos (CSV y JSON) con los fragmentos generados de los PDFs.\n",
    "final_docs = docs_no_split + pdf_chunks\n",
    "\n",
    "# Mostramos la cantidad total de documentos finales listos para usar en el modelo RAG.\n",
    "print(f\"\\nTotal de documentos listos para el RAG (final_docs): {len(final_docs)}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2Ggy4MUY9Z8",
    "outputId": "2120d652-4904-465b-ec88-543bd69c09a3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documentos Tabulares/Estructurados (sin dividir): 289\n",
      "Documentos Largos (a dividir): 5\n",
      "Total de chunks generados de los PDFs: 11\n",
      "\n",
      "Total de documentos listos para el RAG (final_docs): 300\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **7. Cargue y creaci\u00f3n del Modelo de Embeddings y Base de Datos Vectorial**\n",
    "\n",
    "En la siguiente celda de c\u00f3digo se inicializa el modelo de embeddings que seleccionamos: BGE-M3, que transforma los textos en representaciones num\u00e9ricas para facilitar su b\u00fasqueda sem\u00e1ntica.\n",
    "Luego creamos la base de datos vectorial usando la seleccionada po nosotros: ChromaDB, donde se almacenan los embeddings de los documentos procesados.\n",
    "Al final, se configura un retriever, encargado de recuperar los fragmentos m\u00e1s relevantes para el modelo RAG durante la generaci\u00f3n de respuestas."
   ],
   "metadata": {
    "id": "JecXTirYaV7J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oMmJdEiWR7a",
    "outputId": "42b331b3-f7b3-447b-fa31-ade8037a1393"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------\n",
    "# 1. Inicializaci\u00f3n del Modelo de Embedding (BGE-M3)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Definimos el nombre del modelo de embeddings BGE-M3 alojado en Hugging Face.\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "# Indicamos que el modelo debe usar la GPU disponible en Google Colab para acelerar el procesamiento.\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Establecemos que los embeddings generados se normalicen para mantener coherencia en las b\u00fasquedas vectoriales.\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Inicializamos el modelo de embeddings BGE-M3 con los par\u00e1metros definidos.\n",
    "bge_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Confirmamos que el modelo de embeddings fue cargado correctamente.\n",
    "print(\"Modelo BGE-M3 cargado.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Creaci\u00f3n de la Base de Datos Vectorial (ChromaDB)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Informamos que se iniciar\u00e1 la creaci\u00f3n y carga de la base vectorial.\n",
    "print(\"Creando y poblando ChromaDB con BGE-M3 embeddings...\")\n",
    "\n",
    "# Construimos la base de datos vectorial Chroma usando los documentos procesados (final_docs)\n",
    "# y el modelo BGE-M3 para generar los embeddings correspondientes.\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=final_docs,\n",
    "    embedding=bge_embeddings,\n",
    "    collection_name=\"ecomarket_rag_data\",  # Nombre asignado a la colecci\u00f3n\n",
    "    persist_directory=\"./chroma_db\"        # Carpeta local donde se almacenar\u00e1n los vectores\n",
    ")\n",
    "\n",
    "# Creamos un objeto retriever que permitir\u00e1 recuperar los 3 documentos m\u00e1s relevantes\n",
    "# seg\u00fan la similitud sem\u00e1ntica de los embeddings.\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Confirmamos que la base vectorial ChromaDB qued\u00f3 configurada y lista para ser usada en el RAG.\n",
    "print(\"ChromaDB configurada. Modelo RAG creado y listo para ser usado.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6swQsmaaWRb",
    "outputId": "ca913792-9ebd-4855-9a33-fa0d2fc41c05"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modelo BGE-M3 cargado.\n",
      "Creando y poblando ChromaDB con BGE-M3 embeddings...\n",
      "ChromaDB configurada. Modelo RAG creado y listo para ser usado.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **8. Configuraci\u00f3n del modelo Mistral 7B en modo 4-bit**\n",
    "\n",
    "En la siguiente celda de c\u00f3digo configuramos y cargamos el modelo **Mistral 7B Instruct** usando cuantizaci\u00f3n en **4 bits** para optimizar el uso de memoria y permitir su ejecuci\u00f3n en GPU con menos recursos.  \n",
    "Utilizamos `BitsAndBytesConfig` para definir los par\u00e1metros de cuantizaci\u00f3n y creamos un **pipeline** de generaci\u00f3n de texto que nos permitir\u00e1 generar respuestas de manera eficiente, usando como fuente de informaci\u00f3n los documentos procesados en el modelo de RAG que hemos construido.  \n",
    "\n",
    "Tambi\u00e9n se incluyen funciones auxiliares para generar respuestas a partir de un *prompt* y para formatear textos de contexto, simulando un flujo de interacci\u00f3n tipo chat.\n"
   ],
   "metadata": {
    "id": "TZW3a9pKa2x_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ====================================================\n",
    "# Configuraci\u00f3n del LLM (Mistral 7B) en modo 4-bit\n",
    "# ====================================================\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Identificador del modelo Mistral 7B a cargar\n",
    "\n",
    "print(f\"Cargando el modelo cuantizado: {model_id}...\")\n",
    "# Mensaje indicando inicio de carga del modelo\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Configuraci\u00f3n moderna con BitsAndBytesConfig\n",
    "# ----------------------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                # Activar carga del modelo en 4 bits\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Usar float16 para c\u00e1lculos internos\n",
    "    bnb_4bit_use_double_quant=True,        # Aplicar doble cuantizaci\u00f3n para mejorar precisi\u00f3n\n",
    "    bnb_4bit_quant_type=\"nf4\"              # Tipo de cuantizaci\u00f3n NF4 recomendado\n",
    ")\n",
    "# Configura la cuantizaci\u00f3n 4-bit para optimizar memoria y velocidad\n",
    "\n",
    "# Cargamos el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Carga el tokenizador del modelo para convertir texto a tokens\n",
    "\n",
    "# Cargamos el modelo con la configuraci\u00f3n optimizada\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,   # Aplica la configuraci\u00f3n de cuantizaci\u00f3n\n",
    "    device_map=\"auto\",                # Detecta autom\u00e1ticamente la GPU disponible\n",
    ")\n",
    "# Carga el modelo de lenguaje con cuantizaci\u00f3n y asignaci\u00f3n autom\u00e1tica de dispositivos\n",
    "\n",
    "# Creamos el pipeline de generaci\u00f3n de texto\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.3\n",
    ")\n",
    "# Inicializa un pipeline de generaci\u00f3n de texto con par\u00e1metros b\u00e1sicos\n",
    "\n",
    "print(\"\u2705 LLM Mistral 7B cargado exitosamente en modo 4-bit.\")\n",
    "# Mensaje de confirmaci\u00f3n de que el modelo se carg\u00f3 correctamente\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Funci\u00f3n de generaci\u00f3n de respuesta\n",
    "# ----------------------------------------------------\n",
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el modelo Mistral 7B (modo 4-bit).\n",
    "    Aplica el formato de chat y limpia la salida final.\n",
    "    \"\"\"\n",
    "    # Preparamos el prompt en formato tipo chat\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = llm_pipeline.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # Formatea el prompt seg\u00fan el estilo de chat del modelo\n",
    "\n",
    "    # Generamos la respuesta\n",
    "    output = llm_pipeline(\n",
    "        formatted_prompt,\n",
    "        do_sample=True,\n",
    "        pad_token_id=llm_pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "    # Genera texto a partir del prompt formateado\n",
    "\n",
    "    # Extraemos solo el texto de la respuesta (despu\u00e9s del token [/INST])\n",
    "    generated_text = output[0]['generated_text']\n",
    "    response_start_index = generated_text.find(\"[/INST]\")\n",
    "\n",
    "    if response_start_index != -1:\n",
    "        response = generated_text[response_start_index + len(\"[/INST]\"):].strip()\n",
    "    else:\n",
    "        response = generated_text.strip()\n",
    "    # Limpia la salida eliminando la parte del prompt y dejando solo la respuesta\n",
    "\n",
    "    return response\n",
    "# Devuelve la respuesta final como texto limpio\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Funci\u00f3n auxiliar para formatear contexto\n",
    "# ----------------------------------------------------\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Une una lista de objetos Document (contexto) en una sola cadena de texto.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "# Concatena el contenido de varios documentos separ\u00e1ndolos con saltos de l\u00ednea"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453,
     "referenced_widgets": [
      "eaeb6b311cbb41419a18ea3fffdbe322",
      "2091256243f74c9c964c1a99a346c187",
      "ffeb191c9c294af7897a71833d5c8d41",
      "0d81af82854242adb4cd95b24672d490",
      "0684c8f2d36f437ea88bdff92e36d0ec",
      "5fa49b6e93c7410cb9c635639b4515f4",
      "4f3d402a7b5d4668a6524ebb8b03cc12",
      "b7ca1677e0e143749b1808d61932c66e",
      "7ed1be3d234547dc809e856c24ec4332",
      "25a555594ea14666b0d430f25d1e130c",
      "ecd8a93638254317b04f483456ceca94",
      "a6b788b2e5d94f2cac61adcceed5f631",
      "b7f14b92d0014f629cb8593598a320f1",
      "69da7ad567264bf4aff2fbe5e0a246a3",
      "ebb3689136264c83be9bb5571c0bf3a5",
      "f623716379db423c81cfb83aa83dd7f6",
      "3f11f575a04b462a90501b8c7d2926ee",
      "dd7ec0df5e2e43b38484e6a7e4916eb9",
      "28e2b7fc57ba42a4b15bd07816f4df3f",
      "2005d18e394246878b700b36fdd65a0b",
      "998197f524ba4ca6a9ec0e0d19c3652b",
      "575ef8770d7546e79479a7538fabef80",
      "ebb5b76968cd44b6b2a1814ab9fe6802",
      "1791c47e7d454d698d47cafe797c9e2d",
      "ce3ac4f56892469aa8178f79f6ea825c",
      "537c7b5932964b4aaabd94ef46bb6b24",
      "2e34e1deeb2c457e8d36bc8995ad6f16",
      "06b9e43381f84b4e9ac4d6ac12cb2437",
      "bc775494d71a4193b7f3165d4502a031",
      "6679cd7aaaf849a4a9488c1356fcefb9",
      "8b3bc1d5dea9440eb20658e8a734f91d",
      "f88374100cfb487ebb81c9845a74b030",
      "7e26e5c686734021a1bf77dfc26193ba",
      "5687634d6adb403fabbaa53b825d99ba",
      "c6eb58bf90614f65b30aa7f20df84d6f",
      "985f3c01b321474c957ffc1e1caea4ff",
      "d2977049d4ce4f0997776277d34a99f8",
      "4dfb44d7a65148d8a4ef09b76a04f864",
      "7e6bb3f84e184f1a9f6b32ee9b897b8b",
      "7d8bc79985fd442eae17db7e6ec05893",
      "93805ffd4e224154bc88272f49517b57",
      "274fa52f5c3844b585198d4acb7b9dd7",
      "289daa8e2f2847719fe6e802702ec3e0",
      "fa107742612c43d68ad02611c9fbf3da",
      "ae606c9840c1479e87769c0cb648920f",
      "da236e526ada4d528e79fc19b74e1b68",
      "4e031cffbeb046efbba89a73ab02a70e",
      "ebd6f9b09b2a44598343f6cee883be99",
      "771b0132e9774ec78d6163282ff48d5f",
      "7a37a651b42847499e3a22fbd8785c40",
      "420c293ec9694748bb839f8e19c0af30",
      "1ac3469717684eee94272c74a82fd891",
      "bfdb8c43d49644d58725b4e70cc56fed",
      "cea24c126c8f4cc9b8f79b32a61fb603",
      "f8b2b733730345898549f3b2adb214f3",
      "a615e3fcfc2945708b2cbbdd60412846",
      "da9a0686b25142618ad2dceea095fa73",
      "3f431b7f00d04c8b815b5106b67f4cf7",
      "228d86b242b3482788be9621e690b4b5",
      "1cfdf45026d8493ab0a944c449456352",
      "fd96b5ccd7ae43dbb0384fb5f7df3191",
      "c36c362e46c24a669e70303f73c021fd",
      "1cc67e19ddf942ce89cb8974adb18af2",
      "4bc232022b6b4974971fd2608860bbcf",
      "5c7ea489fecc4f98ba324745ecfd633b",
      "7cceb802ea3c44858e25546415dd9b7c",
      "70b7bab7341345a485e8eb9e49e5ff98",
      "c2ca773b124c4ebbbe32ff3008848da0",
      "893b1c1464e645d994becf4293882510",
      "61afb3c782f14335be8f3220c2375e3e",
      "743bab24960d4c2bbce9b2614268b1bf",
      "13d7971c04da49c2a133834b82aaa445",
      "aef14338600c4b73b89152df5ab1add6",
      "87a2b133882949fb907f74eef486f1f4",
      "1929369dc9a740429b2f321fb8ca70f4",
      "1631ff71c619446c8be03aa592798d88",
      "125e3c2b32f343faba4b1f665a08005d",
      "689920f6ecb54f8486f2f3f192c73307",
      "d46219ec8d984ecb80097a6644310304",
      "d29dc8cf585f49909c3eb1c0df98b580",
      "9ebea6338da944e38eb37bfacced4fd4",
      "e83b0b51de2941199e8be1e8d12feec7",
      "c5aee5dd92cb45b98d9b72294901d044",
      "7a3f1627287541c59bb40758a0e39efb",
      "4e617e638ab54e45b1e47b0391857c3c",
      "178d8c36d27444f8b6df8623938ac1f4",
      "0d3706c6e3734794b4b81747b711b885",
      "c02f79ade49b44d4bf1c1ec7ea5ca4ad",
      "ad63a81b0d10403a90c454859df44935",
      "811461d24ac74b7f9bf815617affb219",
      "76ae1aa631ff4f55b9aa9a5b1c9206db",
      "e26163c16d054896b6d729034a79ee6e",
      "23caf82cc84b4b84a2b1509d15d750ba",
      "21ff8d8e9d134a78ab4497ab0972f122",
      "88ea23fc01ad49d88d8b4085ed0ce647",
      "4fd48d62cb1447a5a4cfa181fee109f5",
      "88bf0805e6ae437a9cbd7c12f9e986ed",
      "2bb7b6b698674124b782429d9f13c587",
      "b6b2fb2c5f904a25a0900d0969649b03",
      "acc08391e75f497f8256d51a0e553efb",
      "c5dc8fa433ee4882bfe8158be37b71aa",
      "64795bd2a2e34f23b4628249c83cf381",
      "6cb24edabd4f428f94891fd72c7b01f8",
      "dde2770176af4ea788e4f1d6ba3ca326",
      "2b9c55ec3fe447b6b607d06c71ad481f",
      "64bc34e7d3474dfb8d3a7fc1f517c0cd",
      "341d1f3fd0c14e8e9cce93ed70e10dcb",
      "3cc23cbd1abe423799e41cee5553c0e9",
      "b7775b09929a4ba2ac88bab6d0e89c86",
      "bbb74a1bea414cc0be063e267e3d549d",
      "2136c7cf02b74293951576fd172042cf",
      "b3cbb056d076414e8f914f6c604b0724",
      "4c0724ed79c942a89a3118ecfdd52e83",
      "599d1747e932480da50d798f50cde950",
      "fb76aff07463434f99cc178742c57047",
      "2488fcec84ee46608fbcd1f75a3c44fe",
      "1c6d08553c11467a8b08b1f0ec63746c",
      "7916d378deb8401a97b38e2725f613a8",
      "08c1048307714588810778d0f4a2945f",
      "58bc5392be4a430ebb1f419e9a62c87d",
      "5e92257d0be5491b9bec979be23d449f",
      "a57077a585e4446f86ffcb1d0ba4ffac",
      "b7e5fc62a7bb42059e84bd41ae59e9b4",
      "d8295a7806534ea19ef0f3d4f13d0889",
      "e023ea514dd04fdfb1877517e6d5ca6e",
      "d8a8821449354566b37e81b52cad4408",
      "98b7bc411f704cab9b629ccf214b881a",
      "2d2edec6c33347808e3d13fdb7cac7ab",
      "ab87dace0be642b39b1ba836585ace92",
      "e5233203519a46b995810ad8ee702ca5",
      "883d5e7ae20946d2b7347d00691a483c",
      "e668b5a0c479409f92c7ef3e14f61b61"
     ]
    },
    "id": "0ksfTwordwTO",
    "outputId": "85149faf-2955-4ffd-b4f9-96ed96a05ff1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cargando el modelo cuantizado: mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaeb6b311cbb41419a18ea3fffdbe322"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6b788b2e5d94f2cac61adcceed5f631"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebb5b76968cd44b6b2a1814ab9fe6802"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5687634d6adb403fabbaa53b825d99ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae606c9840c1479e87769c0cb648920f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a615e3fcfc2945708b2cbbdd60412846"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70b7bab7341345a485e8eb9e49e5ff98"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "689920f6ecb54f8486f2f3f192c73307"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad63a81b0d10403a90c454859df44935"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acc08391e75f497f8256d51a0e553efb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2136c7cf02b74293951576fd172042cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a57077a585e4446f86ffcb1d0ba4ffac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 LLM Mistral 7B cargado exitosamente en modo 4-bit.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **9. Funci\u00f3n de Identificaci\u00f3n de Pregunta de estado de Pedidos**\n",
    "\n",
    "Esta funci\u00f3n identifica si la pregunta del usuario contiene un **ID de pedido** (por ejemplo, \u201cpedido 1023\u201d).\n",
    "Extrae ese n\u00famero para realizar una **b\u00fasqueda exacta** en lugar de una b\u00fasqueda sem\u00e1ntica.\n",
    "Como los pedidos y sus estados est\u00e1n cargados en el modelo RAG, esto permite **recuperar la informaci\u00f3n correcta** sin ambig\u00fcedades.\n",
    "En resumen, mejora la **precisi\u00f3n del chatbot** al consultar pedidos espec\u00edficos.\n",
    "\n"
   ],
   "metadata": {
    "id": "gxWiifNbG6sf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_pedido_id(question: str) -> str | None:\n",
    "    \"\"\"Extrae un ID de pedido de 4 d\u00edgitos (ej. 1003) de una pregunta.\"\"\"\n",
    "    # Buscar un patr\u00f3n de 4 d\u00edgitos num\u00e9ricos (\\b asegura que es una palabra completa)\n",
    "    match = re.search(r'\\b(\\d{4})\\b', question)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ],
   "metadata": {
    "id": "fxlp-GGkvGll"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **10. Definici\u00f3n del Prompt y Flujo RAG**\n",
    "\n",
    "En esta secci\u00f3n definimos el **prompt base** que gu\u00eda al modelo en c\u00f3mo responder al usuario usando solo la informaci\u00f3n recuperada desde el **RAG (Retrieval-Augmented Generation)**.  \n",
    "Tambi\u00e9n implementamos la funci\u00f3n `rag_query()`, que ejecuta todo el flujo: extrae el ID de pedido (si existe), recupera los documentos relevantes desde la base vectorial **ChromaDB**, formatea el contexto y genera la respuesta final con el modelo **Mistral 7B**.  \n",
    "\n",
    "El objetivo es asegurar que las respuestas sean **exactas, contextualizadas y sin invenciones**, especialmente cuando se consulta por el estado de un pedido o preguntas referentes a pol\u00edticas de la empresa en temas de devoluciones, garant\u00edas, entre otras.\n"
   ],
   "metadata": {
    "id": "vYzd7GZ0cxas"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "**[REGLA CR\u00cdTICA] \u00a1ALERTA!** NO UTILICES CONOCIMIENTO EXTERNO. Tu \u00fanica fuente de verdad es el CONTEXTO.\n",
    "Eres un agente de servicio al cliente muy amable, profesional y conciso de la empresa EcoMarket.\n",
    "Tu tarea es responder a la pregunta del usuario bas\u00e1ndote **ESTRICTAMENTE** en el CONTEXTO que te voy a proporcionar.\n",
    "Si el CONTEXTO contiene datos tabulares (ej. un registro CSV), debes extraer y citar LITERALMENTE el valor correspondiente al campo 'estado'. No generes una descripci\u00f3n si el estado es un solo t\u00e9rmino (ej. usa 'Entregado', no 'Est\u00e1 en proceso de env\u00edo').\n",
    "Si el contexto NO CONTIENE la informaci\u00f3n necesaria para responder (por ejemplo, el ID de pedido no existe, el producto no es retornable, o el tema no est\u00e1 cubierto en las pol\u00edticas), debes informar amablemente que la informaci\u00f3n no est\u00e1 disponible en este momento y sugerir un canal de contacto directo (como un correo electr\u00f3nico a soporte).\n",
    "**No inventes datos bajo ninguna circunstancia.**\n",
    "\n",
    "**CONTEXTO (Informaci\u00f3n Recuperada de la Base de Datos Vectorial ChromaDB creada):**\n",
    "{context}\n",
    "\n",
    "**PREGUNTA DEL USUARIO:**\n",
    "{question}\n",
    "\n",
    "# INSTRUCCIONES DE RESPUESTA (Gu\u00eda para generar la respuesta):\n",
    "# 1. \u00a0Comienza con un saludo amable (\"Hola! Gracias por contactarnos...\" o similar)\n",
    "# 2. \u00a0Usa un lenguaje natural y conversacional, manteniendo el tono que ten\u00edas en tus prompts originales.\n",
    "# 3. \u00a0**Si es un estado de pedido (CSV)**: S\u00e9 muy espec\u00edfico e incluye el estado, la estimaci\u00f3n y el c\u00f3digo de rastreo, solo en los casos donde el estado es entregado o cancelado, no incluyas codigo de rastreo. **El c\u00f3digo de rastreo debe presentarse como una URL navegable, usando la base 'https://ecomarket-rastreo.com/tracking/' y agregando el c\u00f3digo recuperado al final.**\n",
    "# 4. \u00a0**Si es una devoluci\u00f3n/pol\u00edtica (CSV, PDF, JSON)**: Indica claramente si la acci\u00f3n es posible y resume los plazos o pasos a seguir seg\u00fan el contexto recuperado.\n",
    "# 5.  Si es una pregunta sobre uso de un producto, por ningun motivo agregues un comentario sobre  estado de pedido, ni de inventario o similar. Limita la respuesta a lo que te piden que es como se usa el producto.\n",
    "# 5. \u00a0Mant\u00e9n la respuesta lo m\u00e1s breve posible sin perder el tono profesional y la informaci\u00f3n clave.\n",
    "# 6. Responde **exclusivamente en espa\u00f1ol**, aunque el contexto contenga informaci\u00f3n en otro idioma.\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "# Crear el PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    template=RAG_PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Funci\u00f3n de Encapsulamiento (para ejecuci\u00f3n simple)\n",
    "# ----------------------------------------------------\n",
    "def rag_query(question: str) -> str:\n",
    "    \"\"\"Encapsula todo el flujo RAG: Recuperaci\u00f3n -> Prompting -> Inferencia.\"\"\"\n",
    "\n",
    "# 1. Extracci\u00f3n y Configuraci\u00f3n del Filtro\n",
    "#    Lo hacemos para saber si la pregunta esta asociada al estado de un pedido.\n",
    "    pedido_id = extract_pedido_id(question)\n",
    "    search_filter = {}\n",
    "\n",
    "    if pedido_id:\n",
    "        # Modo H\u00edbrido: Filtrado por un Numero de Pedido + Similitud Sem\u00e1ntica\n",
    "        search_filter = {\"$contains\": pedido_id}\n",
    "        retriever_hibrido = vector_db.as_retriever(\n",
    "        search_kwargs={\"k\": 5, \"where_document\": search_filter})\n",
    "    else:\n",
    "        # Si no hay un numero de Pedido, la b\u00fasqueda opera normalmente (solo sem\u00e1ntica).\n",
    "        retriever_hibrido = vector_db.as_retriever(\n",
    "        search_kwargs={\"k\": 5})\n",
    "    docs = retriever_hibrido.invoke(question)\n",
    "\n",
    "    if not docs and pedido_id:\n",
    "        # Si se busc\u00f3 un ID espec\u00edfico pero no se encontr\u00f3 nada,\n",
    "        # retornamos un mensaje de error directo sin llamar al LLM.\n",
    "        return f\"Hola. Disculpa, no pudimos encontrar ning\u00fan registro asociado al pedido ID {pedido_id} en nuestra base de datos. Por favor, verifica el ID o cont\u00e1ctanos a soporte@ecomarket.com para asistencia.\"\n",
    "\n",
    "    context_formatted = format_docs(docs)\n",
    "\n",
    "    # 3. Formatear prompt y Generar respuesta con LLM\n",
    "    prompt_str = prompt_template.format(context=context_formatted, question=question)\n",
    "    response = generate_response(prompt_str)\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "id": "eCxvWtgocyMF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **11. Pruebas simulando preguntas de Clientes**\n",
    "\n",
    "Llego el momento de la verdad, probemos si despues de:\n",
    "- Construir nuestro modelo RAG con BGE-M3 y base de datos vectorial ChromaDB.\n",
    "- Definido el modelo LLM Mistral-7B, para generar la respuesta usando los documentos entregados por el modelo de RAG creado.\n",
    "\n",
    "Nos genera respuestas esperadas basados en todos los documentos que propusimos de la compa\u00f1ia EcoMarket: Estados de pedidos, Inventario de Productos, Politicas de Devolucion, Politicas de Ventas, Preguntas frecuentes y Manuales de Uso de Productos."
   ],
   "metadata": {
    "id": "gBa7IGErcHnj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **11.1. Pregunta sobre el Estado de Un Pedido**\n",
    "\n",
    "Tenemos una base de datos de Pedidos con ID desde el 1001 al 1015. Podemos cambiar en la pregunta el ID."
   ],
   "metadata": {
    "id": "7mAiF6obMMfN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pregunta_1 = \"\u00bfCu\u00e1l es el estado actual de mi pedido con ID 1013?\"\n",
    "print(f\"\\nConsulta: {pregunta_1}\")\n",
    "respuesta_1 = rag_query(pregunta_1)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA: \")\n",
    "print(\"=\"*40)\n",
    "print(respuesta_1)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Yh3UnwScH_u",
    "outputId": "91fadf4c-9d82-428f-8bc4-1789c99055f0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Consulta: \u00bfCu\u00e1l es el estado actual de mi pedido con ID 1013?\n",
      "\n",
      "========================================\n",
      "RESPUESTA: \n",
      "========================================\n",
      "Hola! Gracias por contactarnos. El estado actual de tu pedido con ID 1013 es 'Entregado'. Puedes ver el estado y el progreso de tu env\u00edo en el siguiente enlace: <https://ecomarket-rastreo.com/tracking/TRK1013>\n",
      "\n",
      "En caso de tener alguna duda adicional o necesitar asistencia adicional, por favor no dudes en contactarnos a soporte@ecomarket.com. Saludos!\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **11.2. Pregunta sobre el Uso de un Producto**\n",
    "\n",
    "Tenemos un documento PDF con instrucciones de uso de los siguientes productos:\n",
    "\n",
    "- Botella Reutilizable de Acero.\n",
    "- Cepillo Dental de Bamb\u00fa.\n",
    "- Bolsa de Tela Reutilizable\n",
    "- Cera Vegana para Muebles\n",
    "- Detergente Ecol\u00f3gico\n",
    "\n",
    "Podemos preguntar entonces como se usa o como se debe usar alguno de estos productos."
   ],
   "metadata": {
    "id": "6LZPSQYJMa51"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- PRUEBA 2: Uso de Producto - Manual de Uso ---\n",
    "pregunta_2 = \"Como uso el cepillo de bambu ?\"\n",
    "print(f\"\\nConsulta: {pregunta_2}\")\n",
    "respuesta_2 = rag_query(pregunta_2)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA: \")\n",
    "print(\"=\"*40)\n",
    "print(respuesta_2)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfH_NOUG76Mv",
    "outputId": "df7ed9a1-33cc-43cb-c2da-3ce7623f2acf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Consulta: Como uso el cepillo de bambu ?\n",
      "\n",
      "========================================\n",
      "RESPUESTA: \n",
      "========================================\n",
      "Hola! Gracias por contactarnos. El cepillo de bamb\u00fa se utiliza para limpiar los dientes y mantenerlos sanos. No hay informaci\u00f3n sobre c\u00f3mo aplicar un cup\u00f3n de descuento en el contexto proporcionado. Si tienes preguntas adicionales o necesitas ayuda con un cup\u00f3n, por favor contacta a nuestro equipo de atenci\u00f3n al cliente a [soporte@ecomarket.com](mailto:soporte@ecomarket.com). Saludos!\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **11.3. Pregunta sobre Devoluci\u00f3n de un Producto**\n",
    "\n",
    "Hay un base de datos de inventario de Productos que incluye el si un producto es retornable o no. Puedes hacer la pregunta para cualquiera de estos productos:\n",
    "\n",
    "Cepillo de bamb\u00fa\n",
    "Pasta dental ecol\u00f3gica\n",
    "Toalla org\u00e1nica\n",
    "Botella ecol\u00f3gica\n",
    "Bolsas reutilizables\n",
    "Detergente natural\n",
    "Jab\u00f3n org\u00e1nico\n",
    "Desodorante ecol\u00f3gico\n",
    "Esponja vegetal\n",
    "Shampoo s\u00f3lido\n",
    "Acondicionador s\u00f3lido\n",
    "Crema hidratante natural\n",
    "Hilo dental natural\n",
    "B\u00e1lsamo labial ecol\u00f3gico"
   ],
   "metadata": {
    "id": "VrnHAsOGNiG6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- PRUEBA 3: Devoluci\u00f3n de un Producto ---\n",
    "pregunta_3 = \"Estoy pensando en comprar un cepillo de bambu y quiero saber si lo puedo devolver. \u00bfEs retornable?\"\n",
    "print(f\"\\nConsulta: {pregunta_3}\")\n",
    "respuesta_3 = rag_query(pregunta_3)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA:\")\n",
    "print(\"=\"*40)\n",
    "print(respuesta_3)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "id": "reBQbY3BoHFV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6bfb30e7-7939-4e7a-ac66-bee7fb7857ae"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Consulta: Estoy pensando en comprar un cepillo de bambu y quiero saber si lo puedo devolver. \u00bfEs retornable?\n",
      "\n",
      "========================================\n",
      "RESPUESTA:\n",
      "========================================\n",
      "Hola! Gracias por contactarnos con tu pregunta. El cepillo de bamb\u00fa que est\u00e1s considerando comprar, seg\u00fan nuestros registros, es un art\u00edculo retornable. Si deseas realizar una devoluci\u00f3n, puedes hacerlo dentro de los 30 d\u00edas siguientes a tu fecha de compra. Para m\u00e1s detalles sobre el proceso de devoluci\u00f3n, puedes visitar nuestra p\u00e1gina web en el siguiente enlace: <https://ecomarket.com/devoluciones>.\n",
      "\n",
      "Si tienes preguntas adicionales o necesitas ayuda con tu pedido, no dudes en contactarnos a trav\u00e9s de nuestro correo electr\u00f3nico [soporte@ecomarket.com](mailto:soporte@ecomarket.com). Estamos aqu\u00ed para ayudarte.\n",
      "\n",
      "Recuerda que para realizar una devoluci\u00f3n, el art\u00edculo debe estar en su estado original y con su embalaje intacto. Adem\u00e1s, debes enviarlo con un correo certificado de env\u00edo para garantizar su seguridad durante el transporte.\n",
      "\n",
      "Si tienes alguna duda adicional sobre el uso del cepillo de bamb\u00fa, no dudes en preguntarnos. Estamos aqu\u00ed para asesorarte en todo lo relacionado con nuestros productos.\n",
      "\n",
      "Saludos!\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **11.4. Pregunta sobre Politicas de Devoluciones**\n",
    "\n",
    "Entre los documentos cargados al Modelo RAG, se incluye un PDF de Politicas de Devoluciones. Lo invitamos a que haga una pregunta referente a este tema y ver que responde el Modelo Mistral-7B usando como su fuente el Modelo RAG construido que incluye el documento mencionado."
   ],
   "metadata": {
    "id": "vsuoJSKDPTFE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pregunta_4 = \"\u00bfCu\u00e1les son los plazos que tengo para iniciar el proceso de devoluci\u00f3n de un producto defectuoso?\"\n",
    "print(f\"\\nConsulta: {pregunta_4}\")\n",
    "respuesta_4 = rag_query(pregunta_4)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA:\")\n",
    "print(\"=\"*40)\n",
    "print(respuesta_4)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "id": "shY4b6-tPTjJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "47eaf4fc-391d-4942-8e98-58c0eb706842"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Consulta: \u00bfCu\u00e1les son los plazos que tengo para iniciar el proceso de devoluci\u00f3n de un producto defectuoso?\n",
      "\n",
      "========================================\n",
      "RESPUESTA:\n",
      "========================================\n",
      "Hola! Gracias por contactarnos. Seg\u00fan nuestras pol\u00edticas, para realizar una devoluci\u00f3n debes de hacerlo dentro de los 30 d\u00edas siguientes a la fecha de entrega del producto. Para iniciar el proceso, puedes acceder a tu \u00e1rea de cliente en nuestra p\u00e1gina web y seleccionar la opci\u00f3n \"Devoluciones\" en el men\u00fa. Luego, selecciona el pedido correspondiente y sigue las instrucciones para crear una solicitud de devoluci\u00f3n. Si el producto est\u00e1 da\u00f1ado o defectuoso, por favor incluye una descripci\u00f3n detallada de la falta o defecto en tu solicitud. Si el estado de tu pedido es \"Entregado\", el plazo para iniciar la devoluci\u00f3n es de 30 d\u00edas contados desde esa fecha. No olvides que debes enviar el producto de regreso a nosotros en el plazo indicado para que puedamos procesar tu solicitud.\n",
      "\n",
      "Si tienes preguntas adicionales o necesitas ayuda en el proceso de devoluci\u00f3n, no dudes en contactarnos a trav\u00e9s de nuestro correo electr\u00f3nico [support@ecomarket.com](mailto:support@ecomarket.com). Estamos aqu\u00ed para ayudarte.\n",
      "\n",
      "Recuerda que para realizar una devoluci\u00f3n, el producto debe estar en su estado original y en el embalaje original, sin da\u00f1os ni marcas. Adem\u00e1s, debes pagar las costos de env\u00edo de regreso a nosotros.\n",
      "\n",
      "Si el producto no es defectuoso o no cumple las condiciones de devoluci\u00f3n, no podemos aceptar la devoluci\u00f3n y no podr\u00e1s obtener un reembolso.\n",
      "\n",
      "Si tienes preguntas adicionales sobre el uso de un producto en particular, por favor consulta la documentaci\u00f3n o el fabricante del producto.\n",
      "\n",
      "Espero que esta informaci\u00f3n te sea \u00fatil. Si tienes preguntas adicionales, no dudes en preguntarme.\n",
      "\n",
      "Atentamente,\n",
      "[Tu Nombre], Agente de\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **11.5. Preguntas Frecuentes (FAQ)**\n",
    "\n",
    "Entre los documentos cargados al Modelo RAG, se incluye un FAQ (Frequently Asked Questions) en formato JSON. Estas son algunas de las preguntas que incluye:\n",
    "\n",
    "- \u00bfRealizan env\u00edos internacionales?\n",
    "- C\u00f3mo obtengo mi factura?\n",
    "- \u00bfPuedo pagar con varias tarjetas?\n",
    "- \u00bfCu\u00e1nto tiempo tengo para devolver un producto?\n",
    "- \u00bfLa garant\u00eda cubre da\u00f1os por mal uso?\n",
    "- \u00bfC\u00f3mo s\u00e9 si un producto es retornable?\n",
    "- \u00bfPuedo tener varias direcciones de env\u00edo?\n",
    "- \u00bfTienen programas de fidelizaci\u00f3n?\n",
    "- \u00bfC\u00f3mo puedo contactar a atenci\u00f3n al cliente?\n",
    "\n",
    "Y esperamos que el Modelo Mistral-7B, usando el Modelo RAG, devuelva la respuesta esperada que debe provenir del archivo FAQ en formato JSON, que fue cargado al modelo RAG.\n"
   ],
   "metadata": {
    "id": "fnBsalrDPT-S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- PRUEBA 5: Preguntas Frecuentes (JSON) ---\n",
    "pregunta_5 = \"\u00bfPuedo pagar con varias tarjetas?\"\n",
    "print(f\"\\nConsulta: {pregunta_5}\")\n",
    "respuesta_5 = rag_query(pregunta_5)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RESPUESTA:\")\n",
    "print(\"=\"*40)\n",
    "print(respuesta_5)\n",
    "print(\"=\"*40 + \"\\n\")"
   ],
   "metadata": {
    "id": "lCgzrw63PUTD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b30249d1-b691-475d-a0cb-9446232c5f6a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Consulta: \u00bfPuedo pagar con varias tarjetas?\n",
      "\n",
      "========================================\n",
      "RESPUESTA:\n",
      "========================================\n",
      "Hola! Gracias por contactarnos. S\u00ed, en EcoMarket puedes realizar una transacci\u00f3n utilizando varias tarjetas de pago, siempre y cuando cada una de ellas tenga su respectivo c\u00f3digo de seguridad y autorizaci\u00f3n. Por favor, recuerda que cada tarjeta debe estar vinculada a un correo electr\u00f3nico y direcci\u00f3n de env\u00edo distinta en tu cuenta de EcoMarket. En caso de tener dudas adicionales, no dudes en contactarnos a trav\u00e9s de nuestro correo electr\u00f3nico a soporte@ecomarket.com. Saludos!\n",
      "========================================\n",
      "\n"
     ]
    }
   ]
  }
 ]
}